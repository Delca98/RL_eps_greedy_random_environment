{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "498cfe6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import turtle\n",
    "import random\n",
    "import numpy as np\n",
    "from keras import Sequential\n",
    "from collections import deque\n",
    "from keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.optimizers import Adam\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import time\n",
    "grid = np.linspace(-240, 240, 49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21830c2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-240., -230., -220., -210., -200., -190., -180., -170., -160.,\n",
       "       -150., -140., -130., -120., -110., -100.,  -90.,  -80.,  -70.,\n",
       "        -60.,  -50.,  -40.,  -30.,  -20.,  -10.,    0.,   10.,   20.,\n",
       "         30.,   40.,   50.,   60.,   70.,   80.,   90.,  100.,  110.,\n",
       "        120.,  130.,  140.,  150.,  160.,  170.,  180.,  190.,  200.,\n",
       "        210.,  220.,  230.,  240.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18ce976c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TurtleGame():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.done = False\n",
    "        self.reward = 0\n",
    "        self.count = 0\n",
    "        self.gameover = False\n",
    "        self.level = 1\n",
    "        \n",
    "        #background\n",
    "        \n",
    "        self.win = turtle.Screen()\n",
    "        self.win.title('Bad')\n",
    "        self.win.bgcolor('Black')\n",
    "        self.win.tracer(0)\n",
    "        self.win.setup(width=600, height = 600)\n",
    "        \n",
    "        #animal\n",
    "        \n",
    "        self.animal = turtle.Turtle()\n",
    "        self.animal.shape('turtle')\n",
    "        self.animal.direction = 'stop'\n",
    "        self.animal.color('yellow')\n",
    "        self.animal.shapesize(stretch_wid=1, stretch_len=1)\n",
    "        self.animal.penup()\n",
    "        self.animal.goto(random.choice(grid),random.choice(grid))\n",
    "        \n",
    "        \n",
    "        #food\n",
    "        self.foods = []\n",
    "        \n",
    "        for _ in range(3):\n",
    "            self.food = turtle.Turtle()\n",
    "            self.food.shape('circle')\n",
    "            self.food.penup()\n",
    "            self.food.color('orange')\n",
    "            \n",
    "            self.food.goto(random.choice(grid),random.choice(grid))\n",
    "            \n",
    "            self.foods.append(self.food)\n",
    "        \n",
    "        #keyboard\n",
    "        \n",
    "        self.win.listen()\n",
    "        self.win.onkeypress(self.animal_right, 'Right')   \n",
    "        self.win.onkeypress(self.animal_left, 'Left')\n",
    "        self.win.onkeypress(self.animal_up, 'Up')   \n",
    "        self.win.onkeypress(self.animal_down, 'Down')\n",
    "        \n",
    "        \n",
    "        #distance\n",
    "        \n",
    "        self.distance = np.abs(self.animal.distance(self.food.xcor(), self.food.ycor()))\n",
    "        \n",
    "        #lives and score\n",
    "        \n",
    "        self.lives = 1\n",
    "        self.score = 0\n",
    "        \n",
    "        #showing lives and score\n",
    "        self.pen = turtle.Turtle()\n",
    "        self.pen.speed(0)\n",
    "        self.pen.color('blue')\n",
    "        self.pen.penup()\n",
    "        self.pen.goto(0,230)\n",
    "        self.pen.pendown()\n",
    "        self.pen.write('Score: {}  Lives:'.format(self.score, self.lives), align = 'center', font=('Courier', 20))\n",
    "        self.pen.hideturtle()\n",
    "        \n",
    "\n",
    "    \n",
    "        # Create a turtle object for drawing the maze\n",
    "        self.penm = turtle.Turtle()\n",
    "        self.penm.speed(0)\n",
    "        self.penm.color(\"blue\")\n",
    "        self.penm.penup()\n",
    "        self.penm.hideturtle()\n",
    "\n",
    "        \n",
    "        #enemies\n",
    "        self.enemies = []\n",
    "        for x in range(25):\n",
    "            self.enemy = turtle.Turtle()\n",
    "            self.enemy.penup()\n",
    "            self.enemy.color('red')\n",
    "            self.enemy.shape('circle')\n",
    "            self.enemy.shapesize(stretch_wid=0.5, stretch_len=0.5)\n",
    "            self.enemy.speed = 10\n",
    "            self.enemy.goto(random.choice(grid),random.choice(grid))\n",
    "            self.enemies.append(self.enemy)\n",
    "            self.enemy.frame_count = 0\n",
    "            \n",
    "    #pacman movement\n",
    "    def movement(self):\n",
    "        \n",
    "        if self.animal.direction == 'right':\n",
    "            \n",
    "            x = self.animal.xcor() \n",
    "\n",
    "            if self.animal.heading() != 0.0:\n",
    "                self.animal.setheading(0.0)\n",
    "\n",
    "            if x < 240:\n",
    "\n",
    "                self.animal.setx(x+10)\n",
    "                \n",
    "        if self.animal.direction == 'left':\n",
    "            \n",
    "            x = self.animal.xcor()   \n",
    "\n",
    "            if self.animal.heading() != 180.0:\n",
    "                self.animal.setheading(180.0)\n",
    "\n",
    "            if x > -240:\n",
    "                self.animal.setx(x-10)  \n",
    "\n",
    "        if self.animal.direction == 'up':\n",
    "            \n",
    "            y = self.animal.ycor()\n",
    "\n",
    "            if self.animal.heading() != 90.0:\n",
    "                self.animal.setheading(90.0)\n",
    "\n",
    "            if y < 240:\n",
    "                self.animal.sety(y+10)\n",
    "            \n",
    "        if self.animal.direction == 'down':\n",
    "            \n",
    "            y = self.animal.ycor()\n",
    "\n",
    "            if self.animal.heading() != 270.0:\n",
    "                self.animal.setheading(270.0)\n",
    "\n",
    "            if y > -240:\n",
    "                \n",
    "                self.animal.sety(y-10)\n",
    "    def move_enemies(self):\n",
    "        \n",
    "        for enemy in self.enemies:\n",
    "            \n",
    "            if enemy.frame_count == 0:\n",
    "                enemy.direction = random.choice(['up', 'down', 'left', 'right'])\n",
    "                enemy.frame_count = random.randint(100, 130)\n",
    "            else:\n",
    "                enemy.frame_count -= 1\n",
    "\n",
    "            if enemy.direction == 'up':\n",
    "                if enemy.ycor() + enemy.speed > 240:\n",
    "                    enemy.direction = 'down'\n",
    "                else:\n",
    "                    enemy.sety(enemy.ycor() + enemy.speed)\n",
    "            elif enemy.direction == 'down':\n",
    "                if enemy.ycor() - enemy.speed < -240:\n",
    "                    enemy.direction = 'up'\n",
    "                else:\n",
    "                    enemy.sety(enemy.ycor() - enemy.speed)\n",
    "            elif enemy.direction == 'left':\n",
    "                if enemy.xcor() - enemy.speed < -240:\n",
    "                    enemy.direction = 'right'\n",
    "                else:\n",
    "                    enemy.setx(enemy.xcor() - enemy.speed)\n",
    "            elif enemy.direction == 'right':\n",
    "                if enemy.xcor() + enemy.speed > 240:\n",
    "                    enemy.direction = 'left'\n",
    "                else:\n",
    "                    enemy.setx(enemy.xcor() + enemy.speed)\n",
    "        \n",
    "    def animal_right(self):\n",
    "        \n",
    "        self.animal.direction = 'right'\n",
    "        \n",
    "    def animal_left(self):\n",
    "        \n",
    "        self.animal.direction = 'left'\n",
    "\n",
    "    def animal_down(self):\n",
    "        \n",
    "        self.animal.direction = 'down'\n",
    "            \n",
    "    def animal_up(self):\n",
    "        \n",
    "        self.animal.direction = 'up'\n",
    "    \n",
    "    #window update\n",
    "    \n",
    "    def run_frame(self):\n",
    "\n",
    "        self.win.update()\n",
    "\n",
    "        for food in self.foods:\n",
    "            if self.animal.distance(food) < 5:\n",
    "                self.foods.remove(food)\n",
    "                food.hideturtle()\n",
    "                self.count += 1\n",
    "                self.score += 1\n",
    "                self.pen.clear()\n",
    "                self.pen.write('Score: {}  Lives: {}'.format(self.score, self.lives), align = 'center', font=('Courier', 20))\n",
    "        \n",
    "            if self.count == 6:\n",
    "                self.done = True\n",
    "                self.reset()\n",
    "                self.count = 0\n",
    "                self.score = 0\n",
    "                self.lives = 1\n",
    "                self.pen.clear()\n",
    "                self.pen.write('Score: {}  Lives: {}'.format(self.score, self.lives), align = 'center', font=('Courier', 20))\n",
    "        self.enemy_movement()\n",
    " \n",
    "    #AI movement\n",
    "    \n",
    "    def reset(self):\n",
    "        \n",
    "        for food in self.foods:\n",
    "\n",
    "            food.hideturtle()\n",
    "            \n",
    "              \n",
    "        #self.animal.goto(random.choice(grid),random.choice(grid))\n",
    "        self.foods = []\n",
    "        \n",
    "        for _ in range(6):\n",
    "            \n",
    "            self.food = turtle.Turtle()\n",
    "            self.food.shape('circle')\n",
    "            self.food.penup()\n",
    "            self.food.color('orange')\n",
    "            \n",
    "            self.food.goto(random.choice(grid),random.choice(grid))\n",
    "            \n",
    "            self.foods.append(self.food)\n",
    "            \n",
    "        return [self.animal.xcor(), self.animal.ycor(), self.food.xcor(), self.food.ycor(), \n",
    "                self.food.xcor() - self.animal.xcor(), self.food.ycor() - self.animal.ycor()]\n",
    "    \n",
    "    def step(self, action):\n",
    "        \n",
    "        self.done = 0\n",
    "        self.reward = 0\n",
    "        \n",
    "        dist_in = np.abs(self.animal.distance(self.food.xcor(), self.food.ycor()))\n",
    "        \n",
    "        if action == 0:\n",
    "            self.animal_left()\n",
    "            self.reward -= 3\n",
    "            if self.animal.xcor() == -240:\n",
    "                self.reward -= 10\n",
    "        if action == 1:\n",
    "            self.animal_right()\n",
    "            self.reward -= 3\n",
    "            if self.animal.xcor() == +240:\n",
    "                self.reward -= 10\n",
    "        if action == 2:\n",
    "            self.animal_down()\n",
    "            self.reward -= 3\n",
    "            if self.animal.ycor() == -240:\n",
    "                self.reward -= 10\n",
    "        if action == 3:\n",
    "            self.animal_up()\n",
    "            self.reward -= 3\n",
    "            if self.animal.ycor() == +240:\n",
    "                self.reward -= 10\n",
    "        self.distance = np.abs(self.animal.distance(self.food.xcor(), self.food.ycor()))\n",
    "        dist_fin = np.abs(self.animal.distance(self.food.xcor(), self.food.ycor()))\n",
    "        \n",
    "        if dist_fin == 0:\n",
    "            self.reward += 400\n",
    "\n",
    "        if (dist_fin - dist_in > 0):\n",
    "            self.reward -= 10\n",
    "            \n",
    "        elif (dist_fin - dist_in < 0):\n",
    "            self.reward += 5\n",
    "       \n",
    "        self.run_frame()\n",
    "\n",
    "        state = [self.animal.xcor(), self.animal.ycor(), self.food.xcor(), self.food.ycor(), \n",
    "                 self.food.xcor() - self.animal.xcor(), self.food.ycor() - self.animal.ycor()]\n",
    "        return state, self.reward, self.done\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68a811a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TclError",
     "evalue": "invalid command name \".!canvas\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTclError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 61\u001b[0m\n\u001b[0;32m     56\u001b[0m     env\u001b[38;5;241m.\u001b[39mpen\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScore: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m  Lives: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(env\u001b[38;5;241m.\u001b[39mscore, env\u001b[38;5;241m.\u001b[39mlives), align \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcenter\u001b[39m\u001b[38;5;124m'\u001b[39m, font\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCourier\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m20\u001b[39m))\n\u001b[0;32m     60\u001b[0m env\u001b[38;5;241m.\u001b[39mmovement()     \n\u001b[1;32m---> 61\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmove_enemies\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 167\u001b[0m, in \u001b[0;36mTurtleGame.move_enemies\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    165\u001b[0m     enemy\u001b[38;5;241m.\u001b[39mdirection \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 167\u001b[0m     \u001b[43menemy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetx\u001b[49m\u001b[43m(\u001b[49m\u001b[43menemy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxcor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43menemy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspeed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow_env\\lib\\turtle.py:1809\u001b[0m, in \u001b[0;36mTNavigator.setx\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m   1793\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetx\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m   1794\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Set the turtle's first coordinate to x\u001b[39;00m\n\u001b[0;32m   1795\u001b[0m \n\u001b[0;32m   1796\u001b[0m \u001b[38;5;124;03m    Argument:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1807\u001b[0m \u001b[38;5;124;03m    (10.00, 240.00)\u001b[39;00m\n\u001b[0;32m   1808\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1809\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_goto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mVec2D\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_position\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow_env\\lib\\turtle.py:3160\u001b[0m, in \u001b[0;36mRawTurtle._goto\u001b[1;34m(self, end)\u001b[0m\n\u001b[0;32m   3152\u001b[0m go_modes \u001b[38;5;241m=\u001b[39m ( \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_drawing,\n\u001b[0;32m   3153\u001b[0m              \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pencolor,\n\u001b[0;32m   3154\u001b[0m              \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pensize,\n\u001b[0;32m   3155\u001b[0m              \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fillpath, \u001b[38;5;28mlist\u001b[39m))\n\u001b[0;32m   3156\u001b[0m screen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscreen\n\u001b[0;32m   3157\u001b[0m undo_entry \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgo\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_position, end, go_modes,\n\u001b[0;32m   3158\u001b[0m               (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrentLineItem,\n\u001b[0;32m   3159\u001b[0m               \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrentLine[:],\n\u001b[1;32m-> 3160\u001b[0m               \u001b[43mscreen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pointlist\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrentLineItem\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m   3161\u001b[0m               \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems[:])\n\u001b[0;32m   3162\u001b[0m               )\n\u001b[0;32m   3163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mundobuffer:\n\u001b[0;32m   3164\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mundobuffer\u001b[38;5;241m.\u001b[39mpush(undo_entry)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow_env\\lib\\turtle.py:754\u001b[0m, in \u001b[0;36mTurtleScreenBase._pointlist\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_pointlist\u001b[39m(\u001b[38;5;28mself\u001b[39m, item):\n\u001b[0;32m    747\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"returns list of coordinate-pairs of points of item\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;124;03m    Example (for insiders):\u001b[39;00m\n\u001b[0;32m    749\u001b[0m \u001b[38;5;124;03m    >>> from turtle import *\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;124;03m    (9.9999999999999982, 0.0)]\u001b[39;00m\n\u001b[0;32m    753\u001b[0m \u001b[38;5;124;03m    >>> \"\"\"\u001b[39;00m\n\u001b[1;32m--> 754\u001b[0m     cl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    755\u001b[0m     pl \u001b[38;5;241m=\u001b[39m [(cl[i], \u001b[38;5;241m-\u001b[39mcl[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(cl), \u001b[38;5;241m2\u001b[39m)]\n\u001b[0;32m    756\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m  pl\n",
      "File \u001b[1;32m<string>:1\u001b[0m, in \u001b[0;36mcoords\u001b[1;34m(self, *args, **kw)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow_env\\lib\\tkinter\\__init__.py:2766\u001b[0m, in \u001b[0;36mCanvas.coords\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   2762\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return a list of coordinates for the item given in ARGS.\"\"\"\u001b[39;00m\n\u001b[0;32m   2763\u001b[0m \u001b[38;5;66;03m# XXX Should use _flatten on args\u001b[39;00m\n\u001b[0;32m   2764\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtk\u001b[38;5;241m.\u001b[39mgetdouble(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m\n\u001b[0;32m   2765\u001b[0m                    \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtk\u001b[38;5;241m.\u001b[39msplitlist(\n\u001b[1;32m-> 2766\u001b[0m            \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcoords\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)]\n",
      "\u001b[1;31mTclError\u001b[0m: invalid command name \".!canvas\""
     ]
    }
   ],
   "source": [
    "env = TurtleGame()\n",
    "while True:\n",
    "    time.sleep(0.1)\n",
    "    env.win.update()\n",
    "    \n",
    "\n",
    "    for food in env.foods:\n",
    "        \n",
    "        if env.animal.distance(food) < 10:\n",
    "            #env.foods.remove(food)\n",
    "            food.goto(random.choice(grid),random.choice(grid))\n",
    "            #env.foods.append(food)\n",
    "            env.count += 1\n",
    "            env.score += 1\n",
    "            env.pen.clear()\n",
    "            env.pen.write('Score: {}  Lives: {} '.format(env.score, env.lives), align = 'center', font=('Courier', 20))\n",
    "\n",
    "#        if env.count == 6:\n",
    "#            env.done = True\n",
    "#            env.reset()\n",
    "#            env.level += 1\n",
    "#            env.count = 0\n",
    "#            env.score = 0\n",
    "#           env.lives = 1\n",
    "#            env.pen.clear()\n",
    "#            env.pen.write('Score: {}  Lives: {} '.format(env.score, env.lives), align = 'center', font=('Courier', 20))\n",
    "    \n",
    "    for enemy in env.enemies:\n",
    "        \n",
    "        if env.animal.distance(enemy) < 5 and env.lives != 0:\n",
    "            \n",
    "            env.lives -= 1\n",
    "            env.pen.clear()\n",
    "            env.pen.write('Score: {}  Lives: {}'.format(env.score, env.lives), align = 'center', font=('Courier', 20))\n",
    "            time.sleep(1)\n",
    "            env.animal.goto(random.choice(grid),random.choice(grid))\n",
    "\n",
    "    if env.lives == 0:\n",
    "        \n",
    "        env.done = True\n",
    "        env.reset()\n",
    "        env.level = 1\n",
    "        env.count = 0\n",
    "        env.score = 0\n",
    "        env.lives = 1\n",
    "        game_over = turtle.Turtle()\n",
    "        game_over.speed(0)\n",
    "        game_over.color('yellow')\n",
    "        game_over.penup()\n",
    "        game_over.hideturtle()\n",
    "        game_over.goto(0, 0)\n",
    "        game_over.write('Game Over', align='center', font=('Courier', 40, 'bold'))\n",
    "        time.sleep(2)\n",
    "        game_over.clear()\n",
    "        env.pen.clear()\n",
    "        env.pen.write('Score: {}  Lives: {}'.format(env.score, env.lives), align = 'center', font=('Courier', 20))\n",
    "\n",
    "        \n",
    "        \n",
    "    env.movement()     \n",
    "    env.move_enemies()\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95d44e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "\n",
    "    def __init__(self, action_space, state_space):\n",
    "\n",
    "        self.action_space = action_space\n",
    "        self.state_space = state_space\n",
    "        self.epsilon = 1\n",
    "        self.gamma = .95\n",
    "        self.batch_size = 64\n",
    "        self.epsilon_min = .01\n",
    "        self.epsilon_decay = .995\n",
    "        self.learning_rate = 0.001\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Dense(64, input_shape=(self.state_space,), activation='relu'))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(self.action_space, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_space)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "    \n",
    "    def act_trained(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # Random action\n",
    "            return random.randrange(self.action_space)\n",
    "        else:\n",
    "            # Greedy action\n",
    "            return np.argmax(self.model.predict(state)[0])\n",
    "\n",
    "\n",
    "    def replay(self):\n",
    "\n",
    "        if len(self.memory) < self.batch_size:\n",
    "\n",
    "            return\n",
    "\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "        states = np.array([i[0] for i in minibatch])\n",
    "        actions = np.array([i[1] for i in minibatch])\n",
    "        rewards = np.array([i[2] for i in minibatch])\n",
    "        next_states = np.array([i[3] for i in minibatch])\n",
    "        dones = np.array([i[4] for i in minibatch])\n",
    "\n",
    "        states = np.squeeze(states)\n",
    "        next_states = np.squeeze(next_states)\n",
    "\n",
    "        targets = rewards + self.gamma*(np.amax(self.model.predict_on_batch(next_states), axis=1))*(1-dones)\n",
    "        targets_full = self.model.predict_on_batch(states)\n",
    "\n",
    "        ind = np.array([i for i in range(self.batch_size)])\n",
    "        targets_full[[ind], [actions]] = targets\n",
    "\n",
    "        self.model.fit(states, targets_full, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "    def save_agent(self, file_path):\n",
    "       \n",
    "        self.model.save(file_path + \"_model.h5\")\n",
    "    \n",
    "        with open(file_path + \"_epsilon.pkl\", \"wb\") as f:\n",
    "            pickle.dump(self.epsilon, f)\n",
    "    \n",
    "    @classmethod\n",
    "    \n",
    "    def load_agent(cls, action_space, state_space, file_path):\n",
    "        \n",
    "        loaded_model = tf.keras.models.load_model(file_path + \"_model.h5\")\n",
    "\n",
    "        with open(file_path + \"_epsilon.pkl\", \"rb\") as f:\n",
    "            \n",
    "            loaded_epsilon = pickle.load(f)\n",
    "            \n",
    "        new_agent = cls(action_space=action_space, state_space=state_space)\n",
    "\n",
    "        new_agent.model = loaded_model\n",
    "        new_agent.epsilon = loaded_epsilon\n",
    "\n",
    "        return new_agent\n",
    "\n",
    "def train_dqn(episode):\n",
    "\n",
    "    loss = []\n",
    "\n",
    "    action_space = 4\n",
    "    state_space = 6\n",
    "    max_steps = 1000\n",
    "\n",
    "    agent = DQN(action_space, state_space)\n",
    "    for e in range(episode):\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, (1, state_space))\n",
    "        score = 0\n",
    "        for i in range(max_steps):\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            score += reward\n",
    "            next_state = np.reshape(next_state, (1, state_space))\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            agent.replay()\n",
    "            if done:\n",
    "                print(\"episode: {}/{}, score: {}\".format(e, episode, score))\n",
    "                break\n",
    "        loss.append(score)\n",
    "    return agent, loss\n",
    "\n",
    "\n",
    "def train_dqn_again(episode, agent):\n",
    "\n",
    "    loss = []\n",
    "\n",
    "    action_space = 4\n",
    "    state_space = 6\n",
    "    max_steps = 1000\n",
    "    \n",
    "    for e in range(episode):\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, (1, state_space))\n",
    "        score = 0\n",
    "        for i in range(max_steps):\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            score += reward\n",
    "            next_state = np.reshape(next_state, (1, state_space))\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            agent.replay()\n",
    "            if done:\n",
    "                print(\"episode: {}/{}, score: {}\".format(e, episode, score))\n",
    "                break\n",
    "        loss.append(score)\n",
    "    return agent, loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test_dqn(episode, agent):\n",
    "    action_space = 4\n",
    "    state_space = 6\n",
    "    max_steps = 100\n",
    "\n",
    "    for e in range(episode):\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, (1, state_space))\n",
    "        score = 0\n",
    "        for i in range(max_steps):\n",
    "            action = agent.act_trained(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            score += reward\n",
    "            next_state = np.reshape(next_state, (1, state_space))\n",
    "            state = next_state\n",
    "            if done:\n",
    "                print(\"episode: {}/{}, score: {}\".format(e, episode, score))\n",
    "                break\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10add63c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e085f505",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
