{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "498cfe6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import turtle\n",
    "import random\n",
    "import numpy as np\n",
    "from keras import Sequential\n",
    "from collections import deque\n",
    "from keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.optimizers import Adam\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import datetime, os\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import psutil\n",
    "import copy\n",
    "grid = np.linspace(-240, 240, 49)\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21830c2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-240., -230., -220., -210., -200., -190., -180., -170., -160.,\n",
       "       -150., -140., -130., -120., -110., -100.,  -90.,  -80.,  -70.,\n",
       "        -60.,  -50.,  -40.,  -30.,  -20.,  -10.,    0.,   10.,   20.,\n",
       "         30.,   40.,   50.,   60.,   70.,   80.,   90.,  100.,  110.,\n",
       "        120.,  130.,  140.,  150.,  160.,  170.,  180.,  190.,  200.,\n",
       "        210.,  220.,  230.,  240.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c4e8438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor_memory():\n",
    "    process = psutil.Process()\n",
    "    memory_info = process.memory_info()\n",
    "    total_memory = psutil.virtual_memory().total\n",
    "    \n",
    "    used_percent = (memory_info.rss / total_memory) * 100\n",
    "    disk_usage = psutil.disk_usage(\"/\")\n",
    "    percent_used = disk_usage.percent\n",
    "    print(f\"RAM Usage: {used_percent:.2f}%\")\n",
    "    print(f\"Disk Space Used: {percent_used:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18ce976c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TurtleGame():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.done = False\n",
    "        self.reward = 0\n",
    "        self.count = 0\n",
    "        self.gameover = False\n",
    "        self.level = 1\n",
    "        self.n = 0\n",
    "        \n",
    "        #sfondo\n",
    "        \n",
    "        self.win = turtle.Screen()\n",
    "        self.win.title('Bad')\n",
    "        self.win.bgcolor('Black')\n",
    "        self.win.tracer(0)\n",
    "        self.win.setup(width=600, height = 600)\n",
    "        \n",
    "        #turtle\n",
    "        \n",
    "        self.animal = turtle.Turtle()\n",
    "        self.animal.shape('turtle')\n",
    "        self.animal.direction = 'stop'\n",
    "        self.animal.color('yellow')\n",
    "        self.animal.shapesize(stretch_wid=1, stretch_len=1)\n",
    "        self.animal.penup()\n",
    "        self.animal.goto(random.choice(grid),random.choice(grid))\n",
    "        \n",
    "        #food\n",
    "        self.foods = []\n",
    "        \n",
    "        for _ in range(3):\n",
    "            self.food = turtle.Turtle()\n",
    "            self.food.shape('circle')\n",
    "            self.food.penup()\n",
    "            self.food.color('orange')\n",
    "            \n",
    "            self.food.goto(random.choice(grid),random.choice(grid))\n",
    "            \n",
    "            self.foods.append(self.food)\n",
    "        \n",
    "        #commands\n",
    "        \n",
    "        self.win.listen()\n",
    "        self.win.onkeypress(self.animal_right, 'Right')   \n",
    "        self.win.onkeypress(self.animal_left, 'Left')\n",
    "        self.win.onkeypress(self.animal_up, 'Up')   \n",
    "        self.win.onkeypress(self.animal_down, 'Down')\n",
    "        \n",
    "        \n",
    "        #distance\n",
    "        \n",
    "        self.distance = np.abs(self.animal.distance(self.food.xcor(), self.food.ycor()))\n",
    "        \n",
    "        #lives and score\n",
    "        \n",
    "        self.lives = 1\n",
    "        self.score = 0\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.pen = turtle.Turtle()\n",
    "        self.pen.speed(0)\n",
    "        self.pen.color('blue')\n",
    "        self.pen.penup()\n",
    "        self.pen.goto(0,230)\n",
    "        self.pen.pendown()\n",
    "        self.pen.write('Score: {}  Lives:'.format(self.score, self.lives), align = 'center', font=('Courier', 20))\n",
    "        self.pen.hideturtle()\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        #enemies\n",
    "        \n",
    "        self.enemies = []\n",
    "        for x in range(25):\n",
    "            self.enemy = turtle.Turtle()\n",
    "            self.enemy.penup()\n",
    "            self.enemy.color('red')\n",
    "            self.enemy.shape('circle')\n",
    "            self.enemy.shapesize(stretch_wid=0.5, stretch_len=0.5)\n",
    "            self.enemy.speed = 10\n",
    "            self.enemy.goto(random.choice(grid),random.choice(grid))\n",
    "            self.enemies.append(self.enemy)\n",
    "            self.enemy.frame_count = 0\n",
    "            self.enemy.direction = random.choice(['up', 'down', 'left', 'right'])\n",
    "            self.enemy.dir = 0\n",
    "            if self.enemy.direction == 'up':\n",
    "                self.enemy.dir = 1.\n",
    "            if self.enemy.direction == 'down':\n",
    "                self.enemy.dir = -1.\n",
    "            if self.enemy.direction == 'left':\n",
    "                self.enemy.dir = 0.5  \n",
    "            if self.enemy.direction == 'right':\n",
    "                self.enemy.dir = -0.5\n",
    "                \n",
    "    #movement\n",
    "    \n",
    "    def movement(self):\n",
    "        \n",
    "        if self.animal.direction == 'right':\n",
    "            \n",
    "            x = self.animal.xcor() \n",
    "\n",
    "            if self.animal.heading() != 0.0:\n",
    "                self.animal.setheading(0.0)\n",
    "\n",
    "            if x < 240:\n",
    "\n",
    "                self.animal.setx(x+10)\n",
    "                \n",
    "        if self.animal.direction == 'left':\n",
    "            \n",
    "            x = self.animal.xcor()   \n",
    "\n",
    "            if self.animal.heading() != 180.0:\n",
    "                self.animal.setheading(180.0)\n",
    "\n",
    "            if x > -240:\n",
    "                self.animal.setx(x-10)  \n",
    "\n",
    "        if self.animal.direction == 'up':\n",
    "            \n",
    "            y = self.animal.ycor()\n",
    "\n",
    "            if self.animal.heading() != 90.0:\n",
    "                self.animal.setheading(90.0)\n",
    "\n",
    "            if y < 240:\n",
    "                self.animal.sety(y+10)\n",
    "            \n",
    "        if self.animal.direction == 'down':\n",
    "            \n",
    "            y = self.animal.ycor()\n",
    "\n",
    "            if self.animal.heading() != 270.0:\n",
    "                self.animal.setheading(270.0)\n",
    "\n",
    "            if y > -240:\n",
    "                \n",
    "                self.animal.sety(y-10)\n",
    "                \n",
    "    \n",
    "    def move_enemies(self):\n",
    "        \n",
    "        for enemy in self.enemies:\n",
    "            \n",
    "            if enemy.frame_count == 0:\n",
    "                enemy.direction = random.choice(['up', 'down', 'left', 'right'])\n",
    "                enemy.frame_count = random.randint(10, 20)\n",
    "            else:\n",
    "                enemy.frame_count -= 1\n",
    "\n",
    "            if enemy.direction == 'up':\n",
    "                if enemy.ycor() + enemy.speed > 240:\n",
    "                    enemy.direction = 'down'\n",
    "                else:\n",
    "                    enemy.sety(enemy.ycor() + enemy.speed)\n",
    "            elif enemy.direction == 'down':\n",
    "                if enemy.ycor() - enemy.speed < -240:\n",
    "                    enemy.direction = 'up'\n",
    "                else:\n",
    "                    enemy.sety(enemy.ycor() - enemy.speed)\n",
    "            elif enemy.direction == 'left':\n",
    "                if enemy.xcor() - enemy.speed < -240:\n",
    "                    enemy.direction = 'right'\n",
    "                else:\n",
    "                    enemy.setx(enemy.xcor() - enemy.speed)\n",
    "            elif enemy.direction == 'right':\n",
    "                if enemy.xcor() + enemy.speed > 240:\n",
    "                    enemy.direction = 'left'\n",
    "                else:\n",
    "                    enemy.setx(enemy.xcor() + enemy.speed)\n",
    "        \n",
    "    def animal_right(self):\n",
    "        \n",
    "        self.animal.direction = 'right'\n",
    "        \n",
    "    def animal_left(self):\n",
    "        \n",
    "        self.animal.direction = 'left'\n",
    "\n",
    "    def animal_down(self):\n",
    "        \n",
    "        self.animal.direction = 'down'\n",
    "            \n",
    "    def animal_up(self):\n",
    "        \n",
    "        self.animal.direction = 'up'\n",
    "    \n",
    "    #window update\n",
    "    \n",
    "    def run_frame(self):\n",
    "        \n",
    "\n",
    "        env.win.update()\n",
    "\n",
    "        for food in env.foods:\n",
    "\n",
    "            if env.animal.distance(food) == 0:\n",
    "                #env.foods.remove(food)\n",
    "                food.goto(random.choice(grid),random.choice(grid))\n",
    "                #env.foods.append(food)\n",
    "                env.count += 1\n",
    "                env.score += 1\n",
    "                env.pen.clear()\n",
    "                env.pen.write('Score: {}  Lives: {} '.format(env.score, env.lives), align = 'center', font=('Courier', 20))\n",
    "\n",
    "    #        if env.count == 6:\n",
    "    #            env.done = True\n",
    "    #            env.reset()\n",
    "    #            env.level += 1\n",
    "    #            env.count = 0\n",
    "    #            env.score = 0\n",
    "    #           env.lives = 1\n",
    "    #            env.pen.clear()\n",
    "    #            env.pen.write('Score: {}  Lives: {} '.format(env.score, env.lives), align = 'center', font=('Courier', 20))\n",
    "\n",
    "        for enemy in env.enemies:\n",
    "\n",
    "            if env.animal.distance(enemy) == 0 and env.lives != 0:\n",
    "\n",
    "                env.lives -= 1\n",
    "                print('Score: ', env.score)\n",
    "                env.pen.clear()\n",
    "                env.pen.write('Score: {}  Lives: {}'.format(env.score, env.lives), align = 'center', font=('Courier', 20))\n",
    "                time.sleep(1)\n",
    "                env.animal.goto(random.choice(grid),random.choice(grid))\n",
    "\n",
    "        if env.lives == 0:\n",
    "\n",
    "            env.done = True\n",
    "            env.reset_good()\n",
    "            env.level = 1\n",
    "            env.count = 0\n",
    "            env.score = 0\n",
    "            env.lives = 1\n",
    "            game_over = turtle.Turtle()\n",
    "            game_over.speed(0)\n",
    "            game_over.color('yellow')\n",
    "            game_over.penup()\n",
    "            game_over.hideturtle()\n",
    "            game_over.goto(0, 0)\n",
    "            game_over.write('Game Over', align='center', font=('Courier', 40, 'bold'))\n",
    "            time.sleep(2)\n",
    "            game_over.clear()\n",
    "            env.pen.clear()\n",
    "            env.pen.write('Score: {}  Lives: {}'.format(env.score, env.lives), align = 'center', font=('Courier', 20))\n",
    "            del game_over\n",
    "        \n",
    "\n",
    "\n",
    "    #AI movement\n",
    "    \n",
    "    def reset(self):\n",
    "        \n",
    "        dist_in = []\n",
    "        \n",
    "            \n",
    "        for food in self.foods:\n",
    "            food.goto(random.choice(grid),random.choice(grid))\n",
    "\n",
    "\n",
    "        self.animal.goto(random.choice(grid),random.choice(grid))\n",
    "        #self.foods = []\n",
    "        \n",
    "#        for _ in range(3):\n",
    "#            \n",
    "#            self.food = turtle.Turtle()\n",
    "#            self.food.shape('circle')\n",
    "#            self.food.penup()\n",
    "#            self.food.color('orange')\n",
    "#            \n",
    "#            self.food.goto(random.choice(grid),random.choice(grid))\n",
    "#            \n",
    "#            self.foods.append(self.food)\n",
    "            \n",
    "        for food in self.foods:\n",
    "            dist_in.append(np.abs(self.animal.distance(food.xcor(), food.ycor())))\n",
    "            \n",
    "        ind_f = np.argmin(dist_in)\n",
    "        \n",
    "\n",
    "        state = [self.animal.xcor()/240, self.animal.ycor()/240,\n",
    "                (self.animal.xcor()-self.foods[ind_f].xcor())/240,\n",
    "                (self.animal.ycor()-self.foods[ind_f].ycor())/240]\n",
    "        \n",
    "        distin_en = []\n",
    "        \n",
    "        for en in self.enemies:\n",
    "            distin_en.append(np.abs(self.animal.distance(en.xcor(), en.ycor())))\n",
    "            \n",
    "        indices = sorted(range(len(distin_en)), key=lambda i: distin_en[i])[:4]\n",
    "        for ind in indices:\n",
    "            if (np.abs(self.animal.xcor()-self.enemies[ind].xcor()) < 51. and np.abs(self.animal.ycor()-self.enemies[ind].ycor())< 51.):\n",
    "                \n",
    "                state.extend([(self.animal.xcor()-self.enemies[ind].xcor())/240, \n",
    "                            (self.animal.ycor()-self.enemies[ind].ycor())/240, self.enemies[ind].dir])\n",
    "            else:\n",
    "                state.extend([1000, 1000, 1000])\n",
    "            \n",
    "        del distin_en\n",
    "        del dist_in\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    \n",
    "    def reset_good(self):\n",
    "        \n",
    "        dist_in = []\n",
    "        \n",
    "            \n",
    "        for food in self.foods:\n",
    "            food.goto(random.choice(grid),random.choice(grid))\n",
    "\n",
    "\n",
    "        self.animal.goto(random.choice(grid),random.choice(grid))\n",
    "        #self.foods = []\n",
    "        \n",
    "#        for _ in range(3):\n",
    "#            \n",
    "#            self.food = turtle.Turtle()\n",
    "#            self.food.shape('circle')\n",
    "#            self.food.penup()\n",
    "#            self.food.color('orange')\n",
    "#            \n",
    "#            self.food.goto(random.choice(grid),random.choice(grid))\n",
    "#            \n",
    "#            self.foods.append(self.food)\n",
    "            \n",
    "        for food in self.foods:\n",
    "            dist_in.append(np.abs(self.animal.distance(food.xcor(), food.ycor())))\n",
    "            \n",
    "        ind_f = np.argmin(dist_in)\n",
    "        \n",
    "\n",
    "        state = [self.animal.xcor()/240, self.animal.ycor()/240,\n",
    "                (self.animal.xcor()-self.foods[ind_f].xcor())/240,\n",
    "                (self.animal.ycor()-self.foods[ind_f].ycor())/240]\n",
    "        \n",
    "        distin_en = []\n",
    "        \n",
    "        for en in self.enemies:\n",
    "            distin_en.append(np.abs(self.animal.distance(en.xcor(), en.ycor())))\n",
    "            \n",
    "        indices = sorted(range(len(distin_en)), key=lambda i: distin_en[i])[:4]\n",
    "        for ind in indices:\n",
    "            \n",
    "            state.extend([(self.animal.xcor()-self.enemies[ind].xcor())/240, \n",
    "                        (self.animal.ycor()-self.enemies[ind].ycor())/240,\n",
    "                         self.enemies[ind].dir/240])\n",
    "        del distin_en\n",
    "        del dist_in\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    \n",
    "    def step(self, action):\n",
    "        \n",
    "        self.done = 0\n",
    "        self.reward = 0\n",
    "        dist_in = []\n",
    "        dist_fin = []\n",
    "        distin_en = []\n",
    "        distfin_en = []\n",
    "        x_in = []\n",
    "        y_in = []\n",
    "        \n",
    "        rew_walls = 2\n",
    "        rew_food_collect = 10.\n",
    "        rew_collision = -20.\n",
    "        rew_seek_food = 3\n",
    "        rew_avoid_food = -3\n",
    "        rew_seek_closer_enemy_far = -1\n",
    "        rew_avoid_closer_enemy_far = +1\n",
    "        rew_seek_closer_enemy_close = -3\n",
    "        rew_avoid_closer_enemy_close = +3\n",
    "        rew_seek_enemy_far = -1\n",
    "        rew_avoid_enemy_far= +1\n",
    "        rew_seek_enemy_close = -2\n",
    "        rew_avoid_enemy_close= +2\n",
    "   \n",
    "        \n",
    "        \n",
    "        for food in self.foods:\n",
    "            dist_in.append(np.abs(self.animal.distance(food.xcor(), food.ycor())))\n",
    "        \n",
    "        \n",
    "        for en in self.enemies:\n",
    "            distin_en.append(np.abs(self.animal.distance(en.xcor(), en.ycor())))\n",
    "            x_in.append(en.xcor())\n",
    "            y_in.append(en.ycor())\n",
    "            en.color('red')\n",
    "\n",
    "        if action == 0:\n",
    "            self.animal_left()\n",
    "            \n",
    "            \n",
    "            if self.animal.xcor() == -240:\n",
    "                self.reward -= rew_walls\n",
    "                \n",
    "        if action == 1:\n",
    "            self.animal_right()\n",
    "            \n",
    "            \n",
    "            if self.animal.xcor() == +240:\n",
    "                self.reward -= rew_walls\n",
    "            \n",
    "        if action == 2:\n",
    "            self.animal_down()\n",
    "            \n",
    "            \n",
    "            if self.animal.ycor() == -240:\n",
    "                self.reward -= rew_walls\n",
    "                \n",
    "        if action == 3:\n",
    "            self.animal_up()\n",
    "        \n",
    "            if self.animal.ycor() == +240:\n",
    "                self.reward -= rew_walls\n",
    "                \n",
    "        ind_f = np.argmin(dist_in)\n",
    "        ind_e = np.argmin(distin_en)        \n",
    "        #self.enemies[ind_e].color('green')\n",
    "        x_en = self.enemies[ind_e].xcor()\n",
    "        y_en = self.enemies[ind_e].ycor()\n",
    "        \n",
    "        indices = sorted(range(len(distin_en)), key=lambda i: distin_en[i])[:4]\n",
    "        self.movement()\n",
    "        self.move_enemies()\n",
    "\n",
    "        \n",
    "        for food in self.foods:\n",
    "            dist_fin.append(np.abs(self.animal.distance(food.xcor(), food.ycor())))\n",
    "            \n",
    "        for en in self.enemies:\n",
    "            distfin_en.append(np.abs(self.animal.distance(en.xcor(), en.ycor())))\n",
    "            \n",
    "        for d in dist_fin:\n",
    "            if d == 0.:\n",
    "                self.reward += rew_food_collect\n",
    "                #self.n = 0\n",
    "                \n",
    "        for d in distfin_en:\n",
    "            if d == 0.:\n",
    "                self.reward += rew_collision\n",
    "                self.done = 1.\n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "        if dist_fin[ind_f] < dist_in[ind_f]:\n",
    "            \n",
    "            self.reward += rew_seek_food\n",
    "        else:\n",
    "            self.reward += rew_avoid_food\n",
    "\n",
    "       #print('la distanza iniziale era', distin_en[ind_e])\n",
    "        #print('la distanza finale Ã¨', self.animal.distance(x_en, y_en))\n",
    "        \n",
    "        if 26. <= distin_en[ind_e] <= 65.:\n",
    "            \n",
    "            \n",
    "            if np.abs(self.animal.distance(x_en, y_en)) < distin_en[ind_e]:\n",
    "            \n",
    "                self.reward += rew_seek_closer_enemy_far\n",
    "               \n",
    "            else:\n",
    "\n",
    "                self.reward += rew_avoid_closer_enemy_far\n",
    "                \n",
    "\n",
    "                \n",
    "        if 0 <=distin_en[ind_e] <= 25:\n",
    "            \n",
    "\n",
    "            if np.abs(self.animal.distance(x_in[ind_e], y_in[ind_e])) <= distin_en[ind_e]:\n",
    "                \n",
    "                self.reward += rew_seek_closer_enemy_close\n",
    "            else:\n",
    "                \n",
    "                self.reward += rew_avoid_closer_enemy_close\n",
    "\n",
    "\n",
    "        state = [self.animal.xcor(), self.animal.ycor(),\n",
    "                (self.animal.xcor()-self.foods[ind_f].xcor())/240,\n",
    "                (self.animal.ycor()-self.foods[ind_f].ycor())/240]\n",
    "        \n",
    "\n",
    "        \n",
    "        for ind in indices:\n",
    "            \n",
    "            if 0 <= distin_en[ind] <= 30:\n",
    "                \n",
    "                if np.abs(self.animal.distance(x_in[ind], y_in[ind])) <= distin_en[ind]:\n",
    "                    \n",
    "                    self.reward += rew_seek_enemy_close\n",
    "                else:\n",
    "                    self.reward += rew_avoid_enemy_close\n",
    "                    \n",
    "            if 30 <= distin_en[ind] <= 65:\n",
    "                \n",
    "                if np.abs(self.animal.distance(x_in[ind], y_in[ind])) <= distin_en[ind]:\n",
    "                    \n",
    "                    self.reward += rew_seek_enemy_far\n",
    "                else:\n",
    "                    self.reward += rew_avoid_enemy_far\n",
    "            \n",
    "            self.enemies[ind].color('green')\n",
    "            #if np.abs(self.animal.distance(self.enemies[ind].xcor(), self.enemies[ind].ycor())) < 20:\n",
    "            #    self.enemies[ind].color('blue')\n",
    "        \n",
    "            \n",
    "                \n",
    "            state.extend([(self.animal.xcor()-self.enemies[ind].xcor())/240, \n",
    "                            (self.animal.ycor()-self.enemies[ind].ycor())/240, self.enemies[ind].dir])\n",
    "\n",
    "            \n",
    "        score = env.score\n",
    "        self.run_frame()\n",
    "        \n",
    "\n",
    "       \n",
    "        #print(self.enemies[ind].dir)\n",
    "        #print(self.reward)\n",
    "        #print(state)\n",
    "        #print('Reward: ', self.reward)\n",
    "        #time.sleep(2)\n",
    "        #print(self.done)\n",
    "        return state, self.reward, self.done, score\n",
    "    \n",
    "    \n",
    "    def step_good(self, action):\n",
    "        \n",
    "        self.done = 0\n",
    "        self.reward = 0\n",
    "        dist_in = []\n",
    "        dist_fin = []\n",
    "        distin_en = []\n",
    "        distfin_en = []\n",
    "        dist_x = []\n",
    "        dist_y = []\n",
    "        \n",
    "        \n",
    "        for food in self.foods:\n",
    "            dist_in.append(np.abs(self.animal.distance(food.xcor(), food.ycor())))\n",
    "        \n",
    "        \n",
    "        for en in self.enemies:\n",
    "            distin_en.append(np.abs(self.animal.distance(en.xcor(), en.ycor())))\n",
    "            dist_x.append(self.animal.xcor()-en.xcor())\n",
    "            dist_y.append(self.animal.ycor()-en.ycor())\n",
    "            en.color('red')\n",
    "\n",
    "        if action == 0:\n",
    "            self.animal_left()\n",
    "            \n",
    "            \n",
    "            if self.animal.xcor() == -240:\n",
    "                self.reward -= 2\n",
    "                \n",
    "        if action == 1:\n",
    "            self.animal_right()\n",
    "            \n",
    "            \n",
    "            if self.animal.xcor() == +240:\n",
    "                self.reward -= 2\n",
    "            \n",
    "        if action == 2:\n",
    "            self.animal_down()\n",
    "            \n",
    "            \n",
    "            if self.animal.ycor() == -240:\n",
    "                self.reward -= 2\n",
    "                \n",
    "        if action == 3:\n",
    "            self.animal_up()\n",
    "        \n",
    "            if self.animal.ycor() == +240:\n",
    "                self.reward -= 2\n",
    "                \n",
    "        ind_f = np.argmin(dist_in)\n",
    "        ind_e = np.argmin(distin_en)        \n",
    "        #self.enemies[ind_e].color('green')\n",
    "        x_en = self.enemies[ind_e].xcor()\n",
    "        y_en = self.enemies[ind_e].ycor()\n",
    "        \n",
    "        self.movement()\n",
    "        self.move_enemies()\n",
    "\n",
    "        \n",
    "        for food in self.foods:\n",
    "            dist_fin.append(np.abs(self.animal.distance(food.xcor(), food.ycor())))\n",
    "            \n",
    "        for en in self.enemies:\n",
    "            distfin_en.append(np.abs(self.animal.distance(en.xcor(), en.ycor())))\n",
    "            \n",
    "        for d in dist_fin:\n",
    "            if d == 0.:\n",
    "                self.reward += 12\n",
    "                #self.n = 0\n",
    "                \n",
    "        for d in distfin_en:\n",
    "            if d == 0.:\n",
    "                self.reward -= 100\n",
    "                self.done = 1.\n",
    "                \n",
    "        \n",
    "        indices = sorted(range(len(distin_en)), key=lambda i: distin_en[i])[:4]\n",
    "        \n",
    "        if dist_fin[ind_f] < dist_in[ind_f]:\n",
    "            \n",
    "            self.reward += 5\n",
    "        else:\n",
    "            self.reward -= 5\n",
    "\n",
    "        \n",
    "        if distin_en[ind_e] <= 50:\n",
    "            \n",
    "            if np.abs(self.animal.distance(x_en, y_en)) <= distin_en[ind_e]:\n",
    "            \n",
    "                self.reward -= 3\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                self.reward += 3\n",
    "\n",
    "                \n",
    "        if distin_en[ind_e] in range(0,21):\n",
    "\n",
    "            if np.abs(self.animal.distance(dist_x[ind_e], dist_y[ind_e])) <= distin_en[ind_e]:\n",
    "                self.reward -= 4\n",
    "            else:\n",
    "                self.reward += 2\n",
    "\n",
    "        state = [self.animal.xcor()/240, self.animal.ycor()/240,\n",
    "                (self.animal.xcor()-self.foods[ind_f].xcor())/240,\n",
    "                (self.animal.ycor()-self.foods[ind_f].ycor())/240]\n",
    "        \n",
    "\n",
    "        \n",
    "        for ind in indices:\n",
    "            \n",
    "            if distin_en[ind] in range(0,21):\n",
    "                \n",
    "                if np.abs(self.animal.distance(dist_x[ind], dist_y[ind])) <= distin_en[ind]:\n",
    "                    self.reward -= 4\n",
    "                else:\n",
    "                    self.reward += 4\n",
    "            if distin_en[ind] in range(21,51):\n",
    "                \n",
    "                if np.abs(self.animal.distance(dist_x[ind], dist_y[ind])) <= distin_en[ind]:\n",
    "                    self.reward -= 1.3\n",
    "                else:\n",
    "                    self.reward += 1.3\n",
    "            \n",
    "            #self.enemies[ind].color('green')\n",
    "            #if np.abs(self.animal.distance(self.enemies[ind].xcor(), self.enemies[ind].ycor())) < 20:\n",
    "            #    self.enemies[ind].color('blue')\n",
    "            \n",
    "            state.extend([(self.animal.xcor()-self.enemies[ind].xcor())/240, \n",
    "                        (self.animal.ycor()-self.enemies[ind].ycor())/240, \n",
    "                          self.enemies[ind].dir/240])\n",
    "        score = env.score\n",
    "        self.run_frame()\n",
    "        \n",
    "        del dist_in \n",
    "        del dist_fin \n",
    "        del distin_en \n",
    "        del distfin_en \n",
    "        del dist_x \n",
    "        del dist_y \n",
    "\n",
    "        #print(state)\n",
    "        #print(self.enemies[ind].dir)\n",
    "        #print(self.reward)\n",
    "        #print(state)\n",
    "        #print('Reward: ', self.reward)\n",
    "        #time.sleep(2)\n",
    "        #print(self.done)\n",
    "        return state, self.reward, self.done, score\n",
    "\n",
    "    def step_alt(self, action):\n",
    "        \n",
    "        self.done = 0\n",
    "        self.reward = 0\n",
    "        dist_in = []\n",
    "        dist_fin = []\n",
    "        disten = []\n",
    "        disten_f = []\n",
    "        x_ini = []\n",
    "        y_ini = []\n",
    "        enemies_inside = []\n",
    "        count = 0\n",
    "        \n",
    "        rew_walls = 2.5\n",
    "        \n",
    "        for food in self.foods:\n",
    "            dist_in.append(np.abs(self.animal.distance(food.xcor(), food.ycor())))\n",
    "        \n",
    "        \n",
    "        for en in self.enemies:\n",
    "            \n",
    "            disten.append(np.abs(self.animal.distance(en.xcor(), en.ycor())))\n",
    "            x_ini.append(en.xcor())\n",
    "            y_ini.append(en.ycor())\n",
    "            en.color('red')\n",
    "            if (np.abs(self.animal.xcor()-en.xcor()) < 51. and np.abs(self.animal.ycor()-en.ycor()) < 51.):\n",
    "                count += 1\n",
    "                enemies_inside.append(self.enemies.index(en))\n",
    "                #print(self.enemies.index(en))\n",
    "                #en.color('green')\n",
    "        \n",
    "        if action == 0:\n",
    "            self.animal_left()\n",
    "            \n",
    "            \n",
    "            if self.animal.xcor() == -240:\n",
    "                self.reward -= rew_walls\n",
    "                \n",
    "        if action == 1:\n",
    "            self.animal_right()\n",
    "            \n",
    "            \n",
    "            if self.animal.xcor() == +240:\n",
    "                self.reward -= rew_walls\n",
    "            \n",
    "        if action == 2:\n",
    "            self.animal_down()\n",
    "            \n",
    "            \n",
    "            if self.animal.ycor() == -240:\n",
    "                self.reward -= rew_walls\n",
    "                \n",
    "        if action == 3:\n",
    "            self.animal_up()\n",
    "        \n",
    "            if self.animal.ycor() == +240:\n",
    "                self.reward -= rew_walls\n",
    "                \n",
    "        ind_f = np.argmin(dist_in)\n",
    "        ind_e = np.argmin(disten)        \n",
    "        #self.enemies[ind_e].color('green')\n",
    "        x_en = self.enemies[ind_e].xcor()\n",
    "        y_en = self.enemies[ind_e].ycor()\n",
    "        \n",
    "        \n",
    "        self.movement()\n",
    "        self.move_enemies()\n",
    "        indices = sorted(range(len(disten)), key=lambda i: disten[i])[:4]\n",
    "        \n",
    "        for food in self.foods:\n",
    "            dist_fin.append(np.abs(self.animal.distance(food.xcor(), food.ycor())))\n",
    "            \n",
    "        for en in self.enemies:\n",
    "            disten_f.append(np.abs(self.animal.distance(en.xcor(), en.ycor())))\n",
    "            \n",
    "        for d in dist_fin:\n",
    "            if d == 0.:\n",
    "                self.reward += 3\n",
    "                \n",
    "                \n",
    "        for d in disten_f:\n",
    "            if d == 0.:\n",
    "                self.reward -= 10\n",
    "                self.done = 1.\n",
    "                \n",
    "        if dist_fin[ind_f] < dist_in[ind_f]:\n",
    "            \n",
    "            self.reward += 2\n",
    "        else:\n",
    "            self.reward -= 2\n",
    "                \n",
    "\n",
    "            \n",
    "\n",
    "        if count == 1:\n",
    "            \n",
    "            \n",
    "            for index in enemies_inside:\n",
    "                \n",
    "                if np.abs(self.animal.distance(x_ini[index], y_ini[index])) > disten[index]:\n",
    "                    self.reward += 2.1\n",
    "                    #print('allontanato')\n",
    "                    \n",
    "                elif np.abs(self.animal.distance(x_ini[index], y_ini[index])) < disten[index]:\n",
    "                    self.reward -= 2\n",
    "                    \n",
    "\n",
    "        if count == 2:\n",
    "            \n",
    "            \n",
    "            for index in enemies_inside:\n",
    "                \n",
    "                if np.abs(self.animal.distance(x_ini[index], y_ini[index])) > disten[index]:\n",
    "                    self.reward += 2\n",
    "                    \n",
    "                elif np.abs(self.animal.distance(x_ini[index], y_ini[index])) < disten[index]:\n",
    "                    self.reward -= 2\n",
    "                    \n",
    "        if count == 3:\n",
    "            \n",
    "            \n",
    "            for index in enemies_inside:\n",
    "                \n",
    "                \n",
    "                if np.abs(self.animal.distance(x_ini[index], y_ini[index])) > disten[index]:\n",
    "                    self.reward += 2\n",
    "                    \n",
    "                elif np.abs(self.animal.distance(x_ini[index], y_ini[index])) < disten[index]:\n",
    "                    self.reward -= 2\n",
    "                    \n",
    "        if count >= 4:\n",
    "            \n",
    "            if np.abs(self.animal.distance(x_en, y_en)) < disten[ind_e]:\n",
    "                self.reward -= 1\n",
    "            elif np.abs(self.animal.distance(x_en, y_en)) > disten[ind_e]:\n",
    "                self.reward += 2\n",
    "            \n",
    "            \n",
    "            \n",
    "            for index in enemies_inside:\n",
    "                \n",
    "                \n",
    "                if np.abs(self.animal.distance(x_ini[index], y_ini[index])) > disten[index]:\n",
    "                    self.reward += 1\n",
    "                    \n",
    "                elif np.abs(self.animal.distance(x_ini[index], y_ini[index])) < disten[index]:\n",
    "                    self.reward -= 1\n",
    "                    \n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "        state = [self.animal.xcor()/240, self.animal.ycor()/240,\n",
    "                (self.animal.xcor()-self.foods[ind_f].xcor())/240,\n",
    "                (self.animal.ycor()-self.foods[ind_f].ycor())/240]\n",
    "        \n",
    "\n",
    "        for ind in indices:\n",
    "            \n",
    "            if (np.abs(self.animal.xcor()-self.enemies[ind].xcor()) < 51. and np.abs(self.animal.ycor()-self.enemies[ind].ycor())< 51.):\n",
    "                \n",
    "                state.extend([(self.animal.xcor()-self.enemies[ind].xcor())/240, \n",
    "                            (self.animal.ycor()-self.enemies[ind].ycor())/240, \n",
    "                              self.enemies[ind].dir])\n",
    "            else:\n",
    "                state.extend([False, False, False])\n",
    "            \n",
    "            \n",
    "            \n",
    "        score = env.score\n",
    "        self.run_frame()\n",
    "        \n",
    "\n",
    "       \n",
    "        #print(self.enemies[ind].dir)\n",
    "        #print('COUNT', count)\n",
    "        #print(self.reward)\n",
    "        #print('Dopo',x_ini)\n",
    "        #print(state)\n",
    "        #print('indici dei nemici ',enemies_inside)\n",
    "        #print('Reward: ', self.reward)\n",
    "        #time.sleep(3)\n",
    "        #print(self.done)\n",
    "        return state, self.reward, self.done, score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa8b87b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_weights(model):\n",
    "    return [layer.get_weights() for layer in model.layers]\n",
    "\n",
    "def compare_weights(weights1, weights2):\n",
    "    return all(np.array_equal(w1, w2) for w1, w2 in zip(weights1, weights2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a95d44e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, action_space, state_space):\n",
    "        self.action_space = action_space\n",
    "        self.state_space = state_space\n",
    "        self.epsilon = 1\n",
    "        self.gamma = 0.90\n",
    "        self.batch_size = 64\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.999992\n",
    "        self.learning_rate = 0.001\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.deaths = deque(maxlen=1000)\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "        self.update_target_network()\n",
    "        self.losses = 0\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(64, input_shape=(self.state_space,), activation='tanh'))\n",
    "        model.add(Dense(128, activation='tanh'))\n",
    "        model.add(Dense(128, activation='tanh'))\n",
    "        model.add(Dense(64, activation='tanh'))\n",
    "        model.add(Dense(self.action_space, activation='linear'))\n",
    "        model.compile(loss='huber_loss', optimizer=Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def remember_prioritized(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        if done:\n",
    "            self.deaths.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_space)\n",
    "        act_values = self.model.predict(state, verbose=0)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def act_trained(self, state):\n",
    " \n",
    "        return np.argmax(self.model.predict(state, verbose=0)[0])\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "        states = np.array([i[0] for i in minibatch])\n",
    "        actions = np.array([i[1] for i in minibatch])\n",
    "        rewards = np.array([i[2] for i in minibatch])\n",
    "        next_states = np.array([i[3] for i in minibatch])\n",
    "        dones = np.array([i[4] for i in minibatch])\n",
    "\n",
    "        states = np.squeeze(states)\n",
    "        next_states = np.squeeze(next_states)\n",
    "\n",
    "        target_q_values = rewards + self.gamma * np.amax(self.target_model.predict_on_batch(next_states), axis=1) * (1 - dones)\n",
    "        q_values = self.model.predict_on_batch(states)\n",
    "\n",
    "        ind = np.array([i for i in range(self.batch_size)])\n",
    "        q_values[[ind], [actions]] = target_q_values\n",
    "\n",
    "        Hist = self.model.fit(states, q_values, epochs=1, verbose=0)\n",
    "        self.losses = Hist.history['loss'][0]\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def replay_prioritized(self):\n",
    "        p = 0.1\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        minibatch = []\n",
    "        for i in range(self.batch_size):\n",
    "            if (len(self.deaths) > 100) and (random.uniform(0, 1) < p):\n",
    "                minibatch.append(random.sample(self.deaths, 1)[0])\n",
    "            else:\n",
    "                minibatch.append(random.sample(self.memory, 1)[0])\n",
    "\n",
    "        states = np.array([i[0] for i in minibatch])\n",
    "        actions = np.array([i[1] for i in minibatch])\n",
    "        rewards = np.array([i[2] for i in minibatch])\n",
    "        next_states = np.array([i[3] for i in minibatch])\n",
    "        dones = np.array([i[4] for i in minibatch])\n",
    "\n",
    "        states = np.squeeze(states)\n",
    "        next_states = np.squeeze(next_states)\n",
    "\n",
    "        target_q_values = rewards + self.gamma * np.amax(self.target_model.predict_on_batch(next_states), axis=1) * (1 - dones)\n",
    "        q_values = self.model.predict_on_batch(states)\n",
    "\n",
    "        ind = np.array([i for i in range(self.batch_size)])\n",
    "        q_values[[ind], [actions]] = target_q_values\n",
    "\n",
    "        Hist = self.model.fit(states, q_values, epochs=1, verbose=0)\n",
    "        self.losses = Hist.history['loss'][0]\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "\n",
    "    def save_agent(self, file_path):\n",
    "        # Save model\n",
    "        self.model.save(file_path + \"_model.h5\")\n",
    "\n",
    "        # Save optimizer state\n",
    "        symbolic_weights = getattr(self.model.optimizer, 'weights')\n",
    "        weight_values = tf.keras.backend.batch_get_value(symbolic_weights)\n",
    "        with open(file_path + '_optimizer.pkl', 'wb') as f:\n",
    "            pickle.dump(weight_values, f)\n",
    "\n",
    "        # Save epsilon and other parameters\n",
    "        with open(file_path + \"_params.pkl\", \"wb\") as f:\n",
    "            pickle.dump({\n",
    "                'epsilon': self.epsilon,\n",
    "                'gamma': self.gamma,\n",
    "                'batch_size': self.batch_size,\n",
    "                'epsilon_min': self.epsilon_min,\n",
    "                'epsilon_decay': self.epsilon_decay,\n",
    "                'learning_rate': self.learning_rate\n",
    "            }, f)\n",
    "        print(\"Agent saved successfully.\")\n",
    "\n",
    "    @classmethod\n",
    "    def load_agent(cls, action_space, state_space, file_path):\n",
    "        # Load model\n",
    "        loaded_model = tf.keras.models.load_model(file_path + \"_model.h5\")\n",
    "\n",
    "        # Load optimizer state\n",
    "        with open(file_path + '_optimizer.pkl', 'rb') as f:\n",
    "            weight_values = pickle.load(f)\n",
    "\n",
    "        # Load epsilon and other parameters\n",
    "        with open(file_path + \"_params.pkl\", \"rb\") as f:\n",
    "            params = pickle.load(f)\n",
    "\n",
    "        new_agent = cls(action_space=action_space, state_space=state_space)\n",
    "        new_agent.model = loaded_model\n",
    "        new_agent.epsilon = params['epsilon']\n",
    "        new_agent.gamma = params['gamma']\n",
    "        new_agent.batch_size = params['batch_size']\n",
    "        new_agent.epsilon_min = params['epsilon_min']\n",
    "        new_agent.epsilon_decay = params['epsilon_decay']\n",
    "        new_agent.learning_rate = params['learning_rate']\n",
    "\n",
    "        # Set optimizer weights\n",
    "        new_agent.model.optimizer.set_weights(weight_values)\n",
    "        print(\"Agent loaded successfully with restored optimizer state.\")\n",
    "\n",
    "        return new_agent\n",
    "    @classmethod\n",
    "    \n",
    "\n",
    "    def load_agent_nopar(cls, action_space, state_space, file_path):\n",
    "        \n",
    "        loaded_model = tf.keras.models.load_model(file_path + \"_model.h5\")\n",
    "\n",
    "        new_agent = cls(action_space=action_space, state_space=state_space)\n",
    "\n",
    "        new_agent.model = loaded_model\n",
    "\n",
    "        return new_agent\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "def log_training_progress(episode, loss, epsilon, loss_history):\n",
    "    print(\"Episode: {:5d}, Loss: {:.5f}, Epsilon: {:.5f}\".format(episode, loss, epsilon))\n",
    "    loss_history.append(loss)\n",
    "\n",
    "def save_step_count(log_dir, step):\n",
    "    step_file = os.path.join(log_dir, 'step_count.json')\n",
    "    with open(step_file, 'w') as f:\n",
    "        json.dump({'step': step}, f)\n",
    "\n",
    "def load_step_count(log_dir):\n",
    "    step_file = os.path.join(log_dir, 'step_count.json')\n",
    "    if os.path.exists(step_file):\n",
    "        with open(step_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            return data['step']\n",
    "    return 0\n",
    "\n",
    "def train_dqn(episode, save_interval=50, target_update_freq=5):\n",
    "    rew_history = []\n",
    "    score_history = []\n",
    "    epsilon_history = []\n",
    "    target_updates = []\n",
    "    Sc = 0\n",
    "\n",
    "    action_space = 4\n",
    "    state_space = 16\n",
    "    max_steps = 1000\n",
    "\n",
    "    agent = DQN(action_space, state_space)\n",
    "\n",
    "    log_dir = 'tb10'\n",
    "    summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "    start_step = load_step_count(log_dir)\n",
    "\n",
    "    for e in range(episode):\n",
    "        state = env.reset_good()\n",
    "        state = np.reshape(state, (1, state_space))\n",
    "        score = 0\n",
    "        episode_losses = []  # To accumulate losses for the episode\n",
    "\n",
    "        for i in range(max_steps):\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, score_info = env.step_good(action)\n",
    "            score += reward\n",
    "            next_state = np.reshape(next_state, (1, state_space))\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            agent.replay()\n",
    "            episode_losses.append(agent.losses)  \n",
    "            if env.score != 0:\n",
    "                Sc = env.score\n",
    "\n",
    "            if done:\n",
    "                print(\"episode: {}/{}, score: {}\".format(e, episode, score))\n",
    "                break\n",
    "        \n",
    "        average_loss = np.mean(episode_losses) \n",
    "        step = start_step + e\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar('reward', score, step=step)\n",
    "            tf.summary.scalar('epsilon', agent.epsilon, step=step)\n",
    "            tf.summary.scalar('score', Sc, step=step)  \n",
    "            tf.summary.scalar('Huber loss', average_loss, step=step)  \n",
    "\n",
    "        score_history.append(Sc)\n",
    "        rew_history.append(score)\n",
    "        epsilon_history.append(agent.epsilon)\n",
    "        Sc = 0\n",
    "\n",
    "        if e % save_interval == 0:\n",
    "            save_path = \"models/episode_{}.h5\".format(e)\n",
    "            agent.save_agent(save_path)\n",
    "            print(\"Saved model at episode {} to {}\".format(e, save_path))\n",
    "\n",
    "        if e % target_update_freq == 0:\n",
    "            agent.update_target_network()\n",
    "            target_updates.append(e)\n",
    "            print(\"Updated target network at episode {}\".format(e))\n",
    "\n",
    "        save_step_count(log_dir, step)\n",
    "\n",
    "    summary_writer.close()\n",
    "\n",
    "    return agent, rew_history\n",
    "\n",
    "def train_dqn_prioritized(episode, save_interval=100):\n",
    "    rew_history = []\n",
    "    score_history = []\n",
    "    epsilon_history = []\n",
    "    Sc = 0\n",
    "\n",
    "    action_space = 4\n",
    "    state_space = 16\n",
    "    max_steps = 1000\n",
    "\n",
    "    agent = DQN(action_space, state_space)\n",
    "\n",
    "    log_dir = 'tb4_prioritized'\n",
    "    summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "    start_step = load_step_count(log_dir)\n",
    "\n",
    "    for e in range(episode):\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, (1, state_space))\n",
    "        score = 0\n",
    "\n",
    "        for i in range(max_steps):\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            score += reward\n",
    "            next_state = np.reshape(next_state, (1, state_space))\n",
    "            agent.remember_prioritized(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            agent.replay_prioritized()\n",
    "            if done:\n",
    "                print(\"episode: {}/{}, score: {}\".format(e, episode, score))\n",
    "                break\n",
    "\n",
    "        step = start_step + e\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar('reward', score, step=step)\n",
    "            tf.summary.scalar('epsilon', agent.epsilon, step=step)\n",
    "            tf.summary.scalar('score', Sc, step=step) \n",
    "            tf.summary.scalar('Huber loss', agent.losses, step=step) \n",
    "\n",
    "        score_history.append(Sc)\n",
    "        rew_history.append(score)\n",
    "        epsilon_history.append(agent.epsilon)\n",
    "        Sc = 0\n",
    "\n",
    "        if e % save_interval == 0:\n",
    "            save_path = \"models/model_episode_{}.h5\".format(e)\n",
    "            agent.save_agent(save_path)\n",
    "            print(\"Saved model at episode {} to {}\".format(e, save_path))\n",
    "\n",
    "        save_step_count(log_dir, step)\n",
    "\n",
    "    summary_writer.close()\n",
    "  \n",
    "\n",
    "    return agent, rew_history\n",
    "\n",
    "def train_dqn_again(episode, agent, save_interval=100, target_update_freq=4):\n",
    "    rew_history = []\n",
    "    score_history = []\n",
    "    epsilon_history = []\n",
    "    target_updates = []\n",
    "    Sc = 0\n",
    "\n",
    "    action_space = 4\n",
    "    state_space = 16\n",
    "    max_steps = 1000\n",
    "\n",
    "    log_dir = 'tb10'\n",
    "    summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "    start_step = load_step_count(log_dir)\n",
    "\n",
    "    for e in range(episode):\n",
    "        state = env.reset_good()\n",
    "        state = np.reshape(state, (1, state_space))\n",
    "        score = 0\n",
    "        episode_losses = []  \n",
    "\n",
    "        for i in range(max_steps):\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, Score = env.step_good(action)\n",
    "            score += reward\n",
    "            next_state = np.reshape(next_state, (1, state_space))\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            agent.replay()\n",
    "            episode_losses.append(agent.losses)  \n",
    "            if env.score != 0:\n",
    "                Sc = env.score\n",
    "\n",
    "            if done:\n",
    "                print(\"episode: {}/{}, score: {}\".format(e, episode, score))\n",
    "                break\n",
    "\n",
    "        average_loss = np.mean(episode_losses)  \n",
    "        step = start_step + e\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar('reward', score, step=step)\n",
    "            tf.summary.scalar('epsilon', agent.epsilon, step=step)\n",
    "            tf.summary.scalar('score', Sc, step=step)\n",
    "            tf.summary.scalar('Huber loss', average_loss, step=step)  \n",
    "\n",
    "        score_history.append(Sc)\n",
    "        rew_history.append(score)\n",
    "        epsilon_history.append(agent.epsilon)\n",
    "        Sc = 0\n",
    "\n",
    "        if e % save_interval == 0:\n",
    "            save_path = \"models/model_episode_{}.h5\".format(e)\n",
    "            agent.save_agent(save_path)\n",
    "            print(\"Saved model at episode {} to {}\".format(e, save_path))\n",
    "\n",
    "        if e % target_update_freq == 0:\n",
    "            agent.update_target_network()\n",
    "            target_updates.append(e)\n",
    "            print(\"Updated target network at episode {}\".format(e))\n",
    "\n",
    "        save_step_count(log_dir, step)\n",
    "\n",
    "    summary_writer.close()\n",
    "\n",
    "    return agent, rew_history\n",
    "\n",
    "def train_dqn_again_prioritized(episode, agent, save_interval=100, target_update_freq=4):\n",
    "    rew_history = []\n",
    "    score_history = []\n",
    "    epsilon_history = []\n",
    "    target_updates = []\n",
    "    Sc = 0\n",
    "\n",
    "    action_space = 4\n",
    "    state_space = 16\n",
    "    max_steps = 1000\n",
    "\n",
    "    log_dir = 'tb10'\n",
    "    summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "    start_step = load_step_count(log_dir)\n",
    "\n",
    "    for e in range(episode):\n",
    "        state = env.reset_good()\n",
    "        state = np.reshape(state, (1, state_space))\n",
    "        score = 0\n",
    "        episode_losses = []  \n",
    "\n",
    "        for i in range(max_steps):\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, score_info = env.step_good(action)\n",
    "            score += reward\n",
    "            next_state = np.reshape(next_state, (1, state_space))\n",
    "            agent.remember_prioritized(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            agent.replay_prioritized()\n",
    "            episode_losses.append(agent.losses)  \n",
    "            if env.score != 0:\n",
    "                Sc = env.score\n",
    "\n",
    "            if done:\n",
    "                print(\"episode: {}/{}, score: {}\".format(e, episode, score))\n",
    "                break\n",
    "\n",
    "        average_loss = np.mean(episode_losses)  \n",
    "        step = start_step + e\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar('reward', score, step=step)\n",
    "            tf.summary.scalar('epsilon', agent.epsilon, step=step)\n",
    "            tf.summary.scalar('score', Sc, step=step)\n",
    "            tf.summary.scalar('Huber loss', average_loss, step=step)  \n",
    "\n",
    "        score_history.append(Sc)\n",
    "        rew_history.append(score)\n",
    "        epsilon_history.append(agent.epsilon)\n",
    "        Sc = 0\n",
    "\n",
    "        if e % save_interval == 0:\n",
    "            save_path = \"models/episode_{}.h5\".format(e)\n",
    "            agent.save_agent(save_path)\n",
    "            print(\"Saved model at episode {} to {}\".format(e, save_path))\n",
    "\n",
    "        if e % target_update_freq == 0:\n",
    "            agent.update_target_network()\n",
    "            target_updates.append(e)\n",
    "            print(\"Updated target network at episode {}\".format(e))\n",
    "\n",
    "        save_step_count(log_dir, step)\n",
    "\n",
    "    summary_writer.close()\n",
    "\n",
    "    return agent, rew_history\n",
    "\n",
    "def test_dqn(episode, agent):\n",
    "    action_space = 4\n",
    "    state_space = 16\n",
    "    max_steps = 1000\n",
    "\n",
    "    for e in range(episode):\n",
    "        state = env.reset_good()\n",
    "        state = np.reshape(state, (1, state_space))\n",
    "        score = 0\n",
    "        for i in range(max_steps):\n",
    "            action = agent.act_trained(state)\n",
    "            next_state, reward, done, Score = env.step_good(action)\n",
    "            score += reward\n",
    "            next_state = np.reshape(next_state, (1, state_space))\n",
    "            state = next_state\n",
    "            if done:\n",
    "                print(\"episode: {}/{}, score: {}\".format(e, episode, score))\n",
    "                break\n",
    "        \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a292fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TurtleGame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de7efb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecee6d62",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-efbfdd2475edd5ca\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-efbfdd2475edd5ca\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 60080;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=tb10 --host \"0.0.0.0\" --port 60080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c94691a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#new_agent,history = train_dqn(501, save_interval=100, target_update_freq=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8601c30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pre_save_weights = get_model_weights(new_agent.model)\n",
    "#agent, loss = train_dqn(300, 100)\n",
    "#new_agent.save_agent('agentmaybe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a51298f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent loaded successfully with restored optimizer state.\n"
     ]
    }
   ],
   "source": [
    "agent_trained = DQN.load_agent(action_space = 4, state_space = 16, file_path = 'agentmaybe6')\n",
    "#post_load_weights = get_model_weights(agent_trained.model)\n",
    "#weights_are_equal = compare_weights(pre_save_weights, post_load_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40eca0b4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  1\n",
      "episode: 0/401, score: -2.0999999999999943\n",
      "Agent saved successfully.\n",
      "Saved model at episode 0 to models/episode_0.h5\n",
      "Updated target network at episode 0\n",
      "Score:  1\n",
      "episode: 1/401, score: 215.10000000000008\n",
      "Score:  5\n",
      "episode: 2/401, score: 661.4999999999998\n"
     ]
    },
    {
     "ename": "TurtleGraphicsError",
     "evalue": "bad color string: red",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTurtleGraphicsError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m agent_trained_1, loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dqn_again_prioritized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m401\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent_trained\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_interval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 403\u001b[0m, in \u001b[0;36mtrain_dqn_again_prioritized\u001b[1;34m(episode, agent, save_interval, target_update_freq)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_steps):\n\u001b[0;32m    402\u001b[0m     action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mact(state)\n\u001b[1;32m--> 403\u001b[0m     next_state, reward, done, score_info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_good\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    404\u001b[0m     score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m    405\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(next_state, (\u001b[38;5;241m1\u001b[39m, state_space))\n",
      "Cell \u001b[1;32mIn[4], line 555\u001b[0m, in \u001b[0;36mTurtleGame.step_good\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    553\u001b[0m     dist_x\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manimal\u001b[38;5;241m.\u001b[39mxcor()\u001b[38;5;241m-\u001b[39men\u001b[38;5;241m.\u001b[39mxcor())\n\u001b[0;32m    554\u001b[0m     dist_y\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manimal\u001b[38;5;241m.\u001b[39mycor()\u001b[38;5;241m-\u001b[39men\u001b[38;5;241m.\u001b[39mycor())\n\u001b[1;32m--> 555\u001b[0m     \u001b[43men\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mred\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    558\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manimal_left()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow_env\\lib\\turtle.py:2217\u001b[0m, in \u001b[0;36mTPen.color\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   2215\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m l \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m   2216\u001b[0m     pcolor \u001b[38;5;241m=\u001b[39m fcolor \u001b[38;5;241m=\u001b[39m args\n\u001b[1;32m-> 2217\u001b[0m pcolor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_colorstr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpcolor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2218\u001b[0m fcolor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_colorstr(fcolor)\n\u001b[0;32m   2219\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpen(pencolor\u001b[38;5;241m=\u001b[39mpcolor, fillcolor\u001b[38;5;241m=\u001b[39mfcolor)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow_env\\lib\\turtle.py:2697\u001b[0m, in \u001b[0;36mRawTurtle._colorstr\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m   2696\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_colorstr\u001b[39m(\u001b[38;5;28mself\u001b[39m, args):\n\u001b[1;32m-> 2697\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscreen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_colorstr\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow_env\\lib\\turtle.py:1159\u001b[0m, in \u001b[0;36mTurtleScreen._colorstr\u001b[1;34m(self, color)\u001b[0m\n\u001b[0;32m   1157\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m color\n\u001b[0;32m   1158\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1159\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TurtleGraphicsError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbad color string: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mstr\u001b[39m(color))\n\u001b[0;32m   1160\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1161\u001b[0m     r, g, b \u001b[38;5;241m=\u001b[39m color\n",
      "\u001b[1;31mTurtleGraphicsError\u001b[0m: bad color string: red"
     ]
    }
   ],
   "source": [
    "agent_trained_1, loss = train_dqn_again_prioritized(401, agent_trained, save_interval = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106acc3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f51f4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_trained_1.save_agent('agentmaybe7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed0960f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b599f4ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c08da026",
   "metadata": {},
   "source": [
    "##### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
