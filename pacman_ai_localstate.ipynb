{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "498cfe6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import turtle\n",
    "import random\n",
    "import numpy as np\n",
    "from keras import Sequential\n",
    "from collections import deque\n",
    "from keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.optimizers import Adam\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import datetime, os\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import psutil\n",
    "import copy\n",
    "grid = np.linspace(-240, 240, 49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21830c2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-240., -230., -220., -210., -200., -190., -180., -170., -160.,\n",
       "       -150., -140., -130., -120., -110., -100.,  -90.,  -80.,  -70.,\n",
       "        -60.,  -50.,  -40.,  -30.,  -20.,  -10.,    0.,   10.,   20.,\n",
       "         30.,   40.,   50.,   60.,   70.,   80.,   90.,  100.,  110.,\n",
       "        120.,  130.,  140.,  150.,  160.,  170.,  180.,  190.,  200.,\n",
       "        210.,  220.,  230.,  240.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c4e8438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor_memory():\n",
    "    process = psutil.Process()\n",
    "    memory_info = process.memory_info()\n",
    "    # Total system memory\n",
    "    total_memory = psutil.virtual_memory().total\n",
    "    \n",
    "    used_percent = (memory_info.rss / total_memory) * 100\n",
    "    disk_usage = psutil.disk_usage(\"/\")\n",
    "    percent_used = disk_usage.percent\n",
    "    print(f\"RAM Usage: {used_percent:.2f}%\")\n",
    "    print(f\"Disk Space Used: {percent_used:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18ce976c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TurtleGame():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.done = False\n",
    "        self.reward = 0\n",
    "        self.count = 0\n",
    "        self.gameover = False\n",
    "        self.level = 1\n",
    "        self.n = 0\n",
    "        #background\n",
    "        \n",
    "        self.win = turtle.Screen()\n",
    "        self.win.title('Bad')\n",
    "        self.win.bgcolor('Black')\n",
    "        self.win.tracer(0)\n",
    "        self.win.setup(width=600, height = 600)\n",
    "        \n",
    "        #animal\n",
    "        \n",
    "        self.animal = turtle.Turtle()\n",
    "        self.animal.shape('turtle')\n",
    "        self.animal.direction = 'stop'\n",
    "        self.animal.color('yellow')\n",
    "        self.animal.shapesize(stretch_wid=1, stretch_len=1)\n",
    "        self.animal.penup()\n",
    "        self.animal.goto(random.choice(grid),random.choice(grid))\n",
    "        #food\n",
    "        self.foods = []\n",
    "        \n",
    "        for _ in range(3):\n",
    "            self.food = turtle.Turtle()\n",
    "            self.food.shape('circle')\n",
    "            self.food.penup()\n",
    "            self.food.color('orange')\n",
    "            \n",
    "            self.food.goto(random.choice(grid),random.choice(grid))\n",
    "            \n",
    "            self.foods.append(self.food)\n",
    "        \n",
    "        #keyboard\n",
    "        \n",
    "        self.win.listen()\n",
    "        self.win.onkeypress(self.animal_right, 'Right')   \n",
    "        self.win.onkeypress(self.animal_left, 'Left')\n",
    "        self.win.onkeypress(self.animal_up, 'Up')   \n",
    "        self.win.onkeypress(self.animal_down, 'Down')\n",
    "        \n",
    "        \n",
    "        #distance\n",
    "        \n",
    "        self.distance = np.abs(self.animal.distance(self.food.xcor(), self.food.ycor()))\n",
    "        \n",
    "        #lives and score\n",
    "        \n",
    "        self.lives = 1\n",
    "        self.score = 0\n",
    "        \n",
    "        #showing lives and score\n",
    "        self.pen = turtle.Turtle()\n",
    "        self.pen.speed(0)\n",
    "        self.pen.color('blue')\n",
    "        self.pen.penup()\n",
    "        self.pen.goto(0,230)\n",
    "        self.pen.pendown()\n",
    "        self.pen.write('Score: {}  Lives:'.format(self.score, self.lives), align = 'center', font=('Courier', 20))\n",
    "        self.pen.hideturtle()\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        #enemies\n",
    "        self.enemies = []\n",
    "        for x in range(25):\n",
    "            self.enemy = turtle.Turtle()\n",
    "            self.enemy.penup()\n",
    "            self.enemy.color('red')\n",
    "            self.enemy.shape('circle')\n",
    "            self.enemy.shapesize(stretch_wid=0.5, stretch_len=0.5)\n",
    "            self.enemy.speed = 10\n",
    "            self.enemy.goto(random.choice(grid),random.choice(grid))\n",
    "            self.enemies.append(self.enemy)\n",
    "            self.enemy.frame_count = 0\n",
    "            self.enemy.direction = random.choice(['up', 'down', 'left', 'right'])\n",
    "            self.enemy.dir = 0\n",
    "            if self.enemy.direction == 'up':\n",
    "                self.enemy.dir = 0.\n",
    "            if self.enemy.direction == 'down':\n",
    "                self.enemy.dir = 1.\n",
    "            if self.enemy.direction == 'left':\n",
    "                self.enemy.dir = 2.   \n",
    "            if self.enemy.direction == 'right':\n",
    "                self.enemy.dir = 3.\n",
    "                \n",
    "    #pacman movement\n",
    "    def movement(self):\n",
    "        \n",
    "        if self.animal.direction == 'right':\n",
    "            \n",
    "            x = self.animal.xcor() \n",
    "\n",
    "            if self.animal.heading() != 0.0:\n",
    "                self.animal.setheading(0.0)\n",
    "\n",
    "            if x < 240:\n",
    "\n",
    "                self.animal.setx(x+10)\n",
    "                \n",
    "        if self.animal.direction == 'left':\n",
    "            \n",
    "            x = self.animal.xcor()   \n",
    "\n",
    "            if self.animal.heading() != 180.0:\n",
    "                self.animal.setheading(180.0)\n",
    "\n",
    "            if x > -240:\n",
    "                self.animal.setx(x-10)  \n",
    "\n",
    "        if self.animal.direction == 'up':\n",
    "            \n",
    "            y = self.animal.ycor()\n",
    "\n",
    "            if self.animal.heading() != 90.0:\n",
    "                self.animal.setheading(90.0)\n",
    "\n",
    "            if y < 240:\n",
    "                self.animal.sety(y+10)\n",
    "            \n",
    "        if self.animal.direction == 'down':\n",
    "            \n",
    "            y = self.animal.ycor()\n",
    "\n",
    "            if self.animal.heading() != 270.0:\n",
    "                self.animal.setheading(270.0)\n",
    "\n",
    "            if y > -240:\n",
    "                \n",
    "                self.animal.sety(y-10)\n",
    "                \n",
    "    \n",
    "    def move_enemies(self):\n",
    "        \n",
    "        for enemy in self.enemies:\n",
    "            \n",
    "            if enemy.frame_count == 0:\n",
    "                enemy.direction = random.choice(['up', 'down', 'left', 'right'])\n",
    "                enemy.frame_count = random.randint(10, 20)\n",
    "            else:\n",
    "                enemy.frame_count -= 1\n",
    "\n",
    "            if enemy.direction == 'up':\n",
    "                if enemy.ycor() + enemy.speed > 240:\n",
    "                    enemy.direction = 'down'\n",
    "                else:\n",
    "                    enemy.sety(enemy.ycor() + enemy.speed)\n",
    "            elif enemy.direction == 'down':\n",
    "                if enemy.ycor() - enemy.speed < -240:\n",
    "                    enemy.direction = 'up'\n",
    "                else:\n",
    "                    enemy.sety(enemy.ycor() - enemy.speed)\n",
    "            elif enemy.direction == 'left':\n",
    "                if enemy.xcor() - enemy.speed < -240:\n",
    "                    enemy.direction = 'right'\n",
    "                else:\n",
    "                    enemy.setx(enemy.xcor() - enemy.speed)\n",
    "            elif enemy.direction == 'right':\n",
    "                if enemy.xcor() + enemy.speed > 240:\n",
    "                    enemy.direction = 'left'\n",
    "                else:\n",
    "                    enemy.setx(enemy.xcor() + enemy.speed)\n",
    "        \n",
    "    def animal_right(self):\n",
    "        \n",
    "        self.animal.direction = 'right'\n",
    "        \n",
    "    def animal_left(self):\n",
    "        \n",
    "        self.animal.direction = 'left'\n",
    "\n",
    "    def animal_down(self):\n",
    "        \n",
    "        self.animal.direction = 'down'\n",
    "            \n",
    "    def animal_up(self):\n",
    "        \n",
    "        self.animal.direction = 'up'\n",
    "    \n",
    "    #window update\n",
    "    \n",
    "    def run_frame(self):\n",
    "        \n",
    "\n",
    "        env.win.update()\n",
    "\n",
    "        for food in env.foods:\n",
    "\n",
    "            if env.animal.distance(food) == 0:\n",
    "                #env.foods.remove(food)\n",
    "                food.goto(random.choice(grid),random.choice(grid))\n",
    "                #env.foods.append(food)\n",
    "                env.count += 1\n",
    "                env.score += 1\n",
    "                env.pen.clear()\n",
    "                env.pen.write('Score: {}  Lives: {} '.format(env.score, env.lives), align = 'center', font=('Courier', 20))\n",
    "\n",
    "    #        if env.count == 6:\n",
    "    #            env.done = True\n",
    "    #            env.reset()\n",
    "    #            env.level += 1\n",
    "    #            env.count = 0\n",
    "    #            env.score = 0\n",
    "    #           env.lives = 1\n",
    "    #            env.pen.clear()\n",
    "    #            env.pen.write('Score: {}  Lives: {} '.format(env.score, env.lives), align = 'center', font=('Courier', 20))\n",
    "\n",
    "        for enemy in env.enemies:\n",
    "\n",
    "            if env.animal.distance(enemy) == 0 and env.lives != 0:\n",
    "\n",
    "                env.lives -= 1\n",
    "                print('Score: ', env.score)\n",
    "                env.pen.clear()\n",
    "                env.pen.write('Score: {}  Lives: {}'.format(env.score, env.lives), align = 'center', font=('Courier', 20))\n",
    "                time.sleep(1)\n",
    "                env.animal.goto(random.choice(grid),random.choice(grid))\n",
    "\n",
    "        if env.lives == 0:\n",
    "\n",
    "            env.done = True\n",
    "            env.reset_good()\n",
    "            env.level = 1\n",
    "            env.count = 0\n",
    "            env.score = 0\n",
    "            env.lives = 1\n",
    "            game_over = turtle.Turtle()\n",
    "            game_over.speed(0)\n",
    "            game_over.color('yellow')\n",
    "            game_over.penup()\n",
    "            game_over.hideturtle()\n",
    "            game_over.goto(0, 0)\n",
    "            game_over.write('Game Over', align='center', font=('Courier', 40, 'bold'))\n",
    "            time.sleep(2)\n",
    "            game_over.clear()\n",
    "            env.pen.clear()\n",
    "            env.pen.write('Score: {}  Lives: {}'.format(env.score, env.lives), align = 'center', font=('Courier', 20))\n",
    "            del game_over\n",
    "        \n",
    "\n",
    "\n",
    "    #AI movement\n",
    "    \n",
    "    def reset(self):\n",
    "        \n",
    "        dist_in = []\n",
    "        \n",
    "            \n",
    "        for food in self.foods:\n",
    "            food.goto(random.choice(grid),random.choice(grid))\n",
    "\n",
    "\n",
    "        self.animal.goto(random.choice(grid),random.choice(grid))\n",
    "        #self.foods = []\n",
    "        \n",
    "#        for _ in range(3):\n",
    "#            \n",
    "#            self.food = turtle.Turtle()\n",
    "#            self.food.shape('circle')\n",
    "#            self.food.penup()\n",
    "#            self.food.color('orange')\n",
    "#            \n",
    "#            self.food.goto(random.choice(grid),random.choice(grid))\n",
    "#            \n",
    "#            self.foods.append(self.food)\n",
    "            \n",
    "        for food in self.foods:\n",
    "            dist_in.append(np.abs(self.animal.distance(food.xcor(), food.ycor())))\n",
    "            \n",
    "        ind_f = np.argmin(dist_in)\n",
    "        \n",
    "\n",
    "        state = [self.animal.xcor()/240, self.animal.ycor()/240,\n",
    "                (self.animal.xcor()-self.foods[ind_f].xcor())/240,\n",
    "                (self.animal.ycor()-self.foods[ind_f].ycor())/240]\n",
    "        \n",
    "        distin_en = []\n",
    "        \n",
    "        for en in self.enemies:\n",
    "            distin_en.append(np.abs(self.animal.distance(en.xcor(), en.ycor())))\n",
    "            \n",
    "        indices = sorted(range(len(distin_en)), key=lambda i: distin_en[i])[:4]\n",
    "        for ind in indices:\n",
    "            if (np.abs(self.animal.xcor()-self.enemies[ind].xcor()) < 51. and np.abs(self.animal.ycor()-self.enemies[ind].ycor())< 51.):\n",
    "                \n",
    "                state.extend([(self.animal.xcor()-self.enemies[ind].xcor())/240, \n",
    "                            (self.animal.ycor()-self.enemies[ind].ycor())/240, self.enemies[ind].dir])\n",
    "            else:\n",
    "                state.extend([1000, 1000, 1000])\n",
    "            \n",
    "        del distin_en\n",
    "        del dist_in\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    \n",
    "    def reset_good(self):\n",
    "        \n",
    "        dist_in = []\n",
    "        \n",
    "            \n",
    "        for food in self.foods:\n",
    "            food.goto(random.choice(grid),random.choice(grid))\n",
    "\n",
    "\n",
    "        self.animal.goto(random.choice(grid),random.choice(grid))\n",
    "        #self.foods = []\n",
    "        \n",
    "#        for _ in range(3):\n",
    "#            \n",
    "#            self.food = turtle.Turtle()\n",
    "#            self.food.shape('circle')\n",
    "#            self.food.penup()\n",
    "#            self.food.color('orange')\n",
    "#            \n",
    "#            self.food.goto(random.choice(grid),random.choice(grid))\n",
    "#            \n",
    "#            self.foods.append(self.food)\n",
    "            \n",
    "        for food in self.foods:\n",
    "            dist_in.append(np.abs(self.animal.distance(food.xcor(), food.ycor())))\n",
    "            \n",
    "        ind_f = np.argmin(dist_in)\n",
    "        \n",
    "\n",
    "        state = [\n",
    "                (self.animal.xcor()-self.foods[ind_f].xcor())/240,\n",
    "                (self.animal.ycor()-self.foods[ind_f].ycor())/240]\n",
    "        \n",
    "        distin_en = []\n",
    "        \n",
    "        for en in self.enemies:\n",
    "            distin_en.append(np.abs(self.animal.distance(en.xcor(), en.ycor())))\n",
    "            \n",
    "        indices = sorted(range(len(distin_en)), key=lambda i: distin_en[i])[:4]\n",
    "        for ind in indices:\n",
    "            state.extend([(self.animal.xcor()-self.enemies[ind].xcor())/240, \n",
    "                        (self.animal.ycor()-self.enemies[ind].ycor())/240])\n",
    "        del distin_en\n",
    "        del dist_in\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    \n",
    "    def step(self, action):\n",
    "        \n",
    "        self.done = 0\n",
    "        self.reward = 0\n",
    "        dist_in = []\n",
    "        dist_fin = []\n",
    "        distin_en = []\n",
    "        distfin_en = []\n",
    "        x_in = []\n",
    "        y_in = []\n",
    "        \n",
    "        rew_walls = 2\n",
    "        rew_food_collect = 10.\n",
    "        rew_collision = -20.\n",
    "        rew_seek_food = 3\n",
    "        rew_avoid_food = -3\n",
    "        rew_seek_closer_enemy_far = -1\n",
    "        rew_avoid_closer_enemy_far = +1\n",
    "        rew_seek_closer_enemy_close = -3\n",
    "        rew_avoid_closer_enemy_close = +3\n",
    "        rew_seek_enemy_far = -1\n",
    "        rew_avoid_enemy_far= +1\n",
    "        rew_seek_enemy_close = -2\n",
    "        rew_avoid_enemy_close= +2\n",
    "   \n",
    "        \n",
    "        \n",
    "        for food in self.foods:\n",
    "            dist_in.append(np.abs(self.animal.distance(food.xcor(), food.ycor())))\n",
    "        \n",
    "        \n",
    "        for en in self.enemies:\n",
    "            distin_en.append(np.abs(self.animal.distance(en.xcor(), en.ycor())))\n",
    "            x_in.append(en.xcor())\n",
    "            y_in.append(en.ycor())\n",
    "            en.color('red')\n",
    "\n",
    "        if action == 0:\n",
    "            self.animal_left()\n",
    "            \n",
    "            \n",
    "            if self.animal.xcor() == -240:\n",
    "                self.reward -= rew_walls\n",
    "                \n",
    "        if action == 1:\n",
    "            self.animal_right()\n",
    "            \n",
    "            \n",
    "            if self.animal.xcor() == +240:\n",
    "                self.reward -= rew_walls\n",
    "            \n",
    "        if action == 2:\n",
    "            self.animal_down()\n",
    "            \n",
    "            \n",
    "            if self.animal.ycor() == -240:\n",
    "                self.reward -= rew_walls\n",
    "                \n",
    "        if action == 3:\n",
    "            self.animal_up()\n",
    "        \n",
    "            if self.animal.ycor() == +240:\n",
    "                self.reward -= rew_walls\n",
    "                \n",
    "        ind_f = np.argmin(dist_in)\n",
    "        ind_e = np.argmin(distin_en)        \n",
    "        #self.enemies[ind_e].color('green')\n",
    "        x_en = self.enemies[ind_e].xcor()\n",
    "        y_en = self.enemies[ind_e].ycor()\n",
    "        \n",
    "        indices = sorted(range(len(distin_en)), key=lambda i: distin_en[i])[:4]\n",
    "        self.movement()\n",
    "        self.move_enemies()\n",
    "\n",
    "        \n",
    "        for food in self.foods:\n",
    "            dist_fin.append(np.abs(self.animal.distance(food.xcor(), food.ycor())))\n",
    "            \n",
    "        for en in self.enemies:\n",
    "            distfin_en.append(np.abs(self.animal.distance(en.xcor(), en.ycor())))\n",
    "            \n",
    "        for d in dist_fin:\n",
    "            if d == 0.:\n",
    "                self.reward += rew_food_collect\n",
    "                #self.n = 0\n",
    "                \n",
    "        for d in distfin_en:\n",
    "            if d == 0.:\n",
    "                self.reward += rew_collision\n",
    "                self.done = 1.\n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "        if dist_fin[ind_f] < dist_in[ind_f]:\n",
    "            \n",
    "            self.reward += rew_seek_food\n",
    "        else:\n",
    "            self.reward += rew_avoid_food\n",
    "\n",
    "       #print('la distanza iniziale era', distin_en[ind_e])\n",
    "        #print('la distanza finale è', self.animal.distance(x_en, y_en))\n",
    "        \n",
    "        if 26. <= distin_en[ind_e] <= 65.:\n",
    "            \n",
    "            \n",
    "            if np.abs(self.animal.distance(x_en, y_en)) < distin_en[ind_e]:\n",
    "            \n",
    "                self.reward += rew_seek_closer_enemy_far\n",
    "               \n",
    "            else:\n",
    "\n",
    "                self.reward += rew_avoid_closer_enemy_far\n",
    "                \n",
    "\n",
    "                \n",
    "        if 0 <=distin_en[ind_e] <= 25:\n",
    "            \n",
    "\n",
    "            if np.abs(self.animal.distance(x_in[ind_e], y_in[ind_e])) <= distin_en[ind_e]:\n",
    "                \n",
    "                self.reward += rew_seek_closer_enemy_close\n",
    "            else:\n",
    "                \n",
    "                self.reward += rew_avoid_closer_enemy_close\n",
    "\n",
    "\n",
    "        state = [self.animal.xcor(), self.animal.ycor(),\n",
    "                (self.animal.xcor()-self.foods[ind_f].xcor())/240,\n",
    "                (self.animal.ycor()-self.foods[ind_f].ycor())/240]\n",
    "        \n",
    "\n",
    "        \n",
    "        for ind in indices:\n",
    "            \n",
    "            if 0 <= distin_en[ind] <= 30:\n",
    "                \n",
    "                if np.abs(self.animal.distance(x_in[ind], y_in[ind])) <= distin_en[ind]:\n",
    "                    \n",
    "                    self.reward += rew_seek_enemy_close\n",
    "                else:\n",
    "                    self.reward += rew_avoid_enemy_close\n",
    "                    \n",
    "            if 30 <= distin_en[ind] <= 65:\n",
    "                \n",
    "                if np.abs(self.animal.distance(x_in[ind], y_in[ind])) <= distin_en[ind]:\n",
    "                    \n",
    "                    self.reward += rew_seek_enemy_far\n",
    "                else:\n",
    "                    self.reward += rew_avoid_enemy_far\n",
    "            \n",
    "            self.enemies[ind].color('green')\n",
    "            #if np.abs(self.animal.distance(self.enemies[ind].xcor(), self.enemies[ind].ycor())) < 20:\n",
    "            #    self.enemies[ind].color('blue')\n",
    "        \n",
    "            \n",
    "                \n",
    "            state.extend([(self.animal.xcor()-self.enemies[ind].xcor())/240, \n",
    "                            (self.animal.ycor()-self.enemies[ind].ycor())/240, self.enemies[ind].dir])\n",
    "\n",
    "            \n",
    "        score = env.score\n",
    "        self.run_frame()\n",
    "        \n",
    "\n",
    "       \n",
    "        #print(self.enemies[ind].dir)\n",
    "        #print(self.reward)\n",
    "        #print(state)\n",
    "        #print('Reward: ', self.reward)\n",
    "        #time.sleep(2)\n",
    "        #print(self.done)\n",
    "        return state, self.reward, self.done, score\n",
    "    \n",
    "    \n",
    "    def step_good(self, action):\n",
    "        \n",
    "        self.done = 0\n",
    "        self.reward = 0\n",
    "        dist_in = []\n",
    "        dist_fin = []\n",
    "        distin_en = []\n",
    "        distfin_en = []\n",
    "        dist_x = []\n",
    "        dist_y = []\n",
    "        \n",
    "        \n",
    "        for food in self.foods:\n",
    "            dist_in.append(np.abs(self.animal.distance(food.xcor(), food.ycor())))\n",
    "        \n",
    "        \n",
    "        for en in self.enemies:\n",
    "            distin_en.append(np.abs(self.animal.distance(en.xcor(), en.ycor())))\n",
    "            dist_x.append(self.animal.xcor()-en.xcor())\n",
    "            dist_y.append(self.animal.ycor()-en.ycor())\n",
    "            en.color('red')\n",
    "\n",
    "        if action == 0:\n",
    "            self.animal_left()\n",
    "            \n",
    "            \n",
    "            if self.animal.xcor() == -240:\n",
    "                self.reward -= 2\n",
    "                \n",
    "        if action == 1:\n",
    "            self.animal_right()\n",
    "            \n",
    "            \n",
    "            if self.animal.xcor() == +240:\n",
    "                self.reward -= 2\n",
    "            \n",
    "        if action == 2:\n",
    "            self.animal_down()\n",
    "            \n",
    "            \n",
    "            if self.animal.ycor() == -240:\n",
    "                self.reward -= 2\n",
    "                \n",
    "        if action == 3:\n",
    "            self.animal_up()\n",
    "        \n",
    "            if self.animal.ycor() == +240:\n",
    "                self.reward -= 2\n",
    "                \n",
    "        ind_f = np.argmin(dist_in)\n",
    "        ind_e = np.argmin(distin_en)        \n",
    "        #self.enemies[ind_e].color('green')\n",
    "        x_en = self.enemies[ind_e].xcor()\n",
    "        y_en = self.enemies[ind_e].ycor()\n",
    "        \n",
    "        self.movement()\n",
    "        self.move_enemies()\n",
    "\n",
    "        \n",
    "        for food in self.foods:\n",
    "            dist_fin.append(np.abs(self.animal.distance(food.xcor(), food.ycor())))\n",
    "            \n",
    "        for en in self.enemies:\n",
    "            distfin_en.append(np.abs(self.animal.distance(en.xcor(), en.ycor())))\n",
    "            \n",
    "        for d in dist_fin:\n",
    "            if d == 0.:\n",
    "                self.reward += 12\n",
    "                #self.n = 0\n",
    "                \n",
    "        for d in distfin_en:\n",
    "            if d == 0.:\n",
    "                self.reward -= 100\n",
    "                self.done = 1.\n",
    "                \n",
    "        \n",
    "        indices = sorted(range(len(distin_en)), key=lambda i: distin_en[i])[:4]\n",
    "        \n",
    "        if dist_fin[ind_f] < dist_in[ind_f]:\n",
    "            \n",
    "            self.reward += 2.2\n",
    "        else:\n",
    "            self.reward -= 2.2\n",
    "\n",
    "        \n",
    "        if distin_en[ind_e] <= 50:\n",
    "            \n",
    "            if np.abs(self.animal.distance(x_en, y_en)) <= distin_en[ind_e]:\n",
    "            \n",
    "                self.reward -= 3\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                self.reward += 3\n",
    "\n",
    "                \n",
    "        if distin_en[ind_e] in range(0,21):\n",
    "\n",
    "            if np.abs(self.animal.distance(dist_x[ind_e], dist_y[ind_e])) <= distin_en[ind_e]:\n",
    "                self.reward -= 4\n",
    "            else:\n",
    "                self.reward += 2\n",
    "\n",
    "        state = [\n",
    "                (self.animal.xcor()-self.foods[ind_f].xcor())/240,\n",
    "                (self.animal.ycor()-self.foods[ind_f].ycor())/240]\n",
    "        \n",
    "\n",
    "        \n",
    "        for ind in indices:\n",
    "            \n",
    "            if distin_en[ind] in range(0,21):\n",
    "                \n",
    "                if np.abs(self.animal.distance(dist_x[ind], dist_y[ind])) <= distin_en[ind]:\n",
    "                    self.reward -= 5\n",
    "                else:\n",
    "                    self.reward += 5\n",
    "            if distin_en[ind] in range(21,51):\n",
    "                \n",
    "                if np.abs(self.animal.distance(dist_x[ind], dist_y[ind])) <= distin_en[ind]:\n",
    "                    self.reward -= 2\n",
    "                else:\n",
    "                    self.reward += 2\n",
    "            \n",
    "            self.enemies[ind].color('green')\n",
    "            #if np.abs(self.animal.distance(self.enemies[ind].xcor(), self.enemies[ind].ycor())) < 20:\n",
    "            #    self.enemies[ind].color('blue')\n",
    "        \n",
    "            state.extend([(self.animal.xcor()-self.enemies[ind].xcor())/240, \n",
    "                        (self.animal.ycor()-self.enemies[ind].ycor())/240])\n",
    "        score = env.score\n",
    "        self.run_frame()\n",
    "        \n",
    "        del dist_in \n",
    "        del dist_fin \n",
    "        del distin_en \n",
    "        del distfin_en \n",
    "        del dist_x \n",
    "        del dist_y \n",
    "\n",
    "\n",
    "        #print(self.enemies[ind].dir)\n",
    "        #print(self.reward)\n",
    "        #print(state)\n",
    "        #print('Reward: ', self.reward)\n",
    "        #time.sleep(2)\n",
    "        #print(self.done)\n",
    "        return state, self.reward, self.done, score\n",
    "\n",
    "    def step_alt(self, action):\n",
    "        \n",
    "        self.done = 0\n",
    "        self.reward = 0\n",
    "        dist_in = []\n",
    "        dist_fin = []\n",
    "        disten = []\n",
    "        disten_f = []\n",
    "        x_ini = []\n",
    "        y_ini = []\n",
    "        enemies_inside = []\n",
    "        count = 0\n",
    "        \n",
    "        rew_walls = 2.5\n",
    "        \n",
    "        for food in self.foods:\n",
    "            dist_in.append(np.abs(self.animal.distance(food.xcor(), food.ycor())))\n",
    "        \n",
    "        \n",
    "        for en in self.enemies:\n",
    "            \n",
    "            disten.append(np.abs(self.animal.distance(en.xcor(), en.ycor())))\n",
    "            x_ini.append(en.xcor())\n",
    "            y_ini.append(en.ycor())\n",
    "            en.color('red')\n",
    "            if (np.abs(self.animal.xcor()-en.xcor()) < 51. and np.abs(self.animal.ycor()-en.ycor()) < 51.):\n",
    "                count += 1\n",
    "                enemies_inside.append(self.enemies.index(en))\n",
    "                #print(self.enemies.index(en))\n",
    "                en.color('green')\n",
    "        \n",
    "        if action == 0:\n",
    "            self.animal_left()\n",
    "            \n",
    "            \n",
    "            if self.animal.xcor() == -240:\n",
    "                self.reward -= rew_walls\n",
    "                \n",
    "        if action == 1:\n",
    "            self.animal_right()\n",
    "            \n",
    "            \n",
    "            if self.animal.xcor() == +240:\n",
    "                self.reward -= rew_walls\n",
    "            \n",
    "        if action == 2:\n",
    "            self.animal_down()\n",
    "            \n",
    "            \n",
    "            if self.animal.ycor() == -240:\n",
    "                self.reward -= rew_walls\n",
    "                \n",
    "        if action == 3:\n",
    "            self.animal_up()\n",
    "        \n",
    "            if self.animal.ycor() == +240:\n",
    "                self.reward -= rew_walls\n",
    "                \n",
    "        ind_f = np.argmin(dist_in)\n",
    "        ind_e = np.argmin(disten)        \n",
    "        #self.enemies[ind_e].color('green')\n",
    "        x_en = self.enemies[ind_e].xcor()\n",
    "        y_en = self.enemies[ind_e].ycor()\n",
    "        \n",
    "        \n",
    "        self.movement()\n",
    "        self.move_enemies()\n",
    "        indices = sorted(range(len(disten)), key=lambda i: disten[i])[:4]\n",
    "        \n",
    "        for food in self.foods:\n",
    "            dist_fin.append(np.abs(self.animal.distance(food.xcor(), food.ycor())))\n",
    "            \n",
    "        for en in self.enemies:\n",
    "            disten_f.append(np.abs(self.animal.distance(en.xcor(), en.ycor())))\n",
    "            \n",
    "        for d in dist_fin:\n",
    "            if d == 0.:\n",
    "                self.reward += 3\n",
    "                \n",
    "                \n",
    "        for d in disten_f:\n",
    "            if d == 0.:\n",
    "                self.reward -= 10\n",
    "                self.done = 1.\n",
    "                \n",
    "        if dist_fin[ind_f] < dist_in[ind_f]:\n",
    "            \n",
    "            self.reward += 2\n",
    "        else:\n",
    "            self.reward -= 2\n",
    "                \n",
    "\n",
    "            \n",
    "\n",
    "        if count == 1:\n",
    "            \n",
    "            \n",
    "            for index in enemies_inside:\n",
    "                \n",
    "                if np.abs(self.animal.distance(x_ini[index], y_ini[index])) > disten[index]:\n",
    "                    self.reward += 2.1\n",
    "                    #print('allontanato')\n",
    "                    \n",
    "                elif np.abs(self.animal.distance(x_ini[index], y_ini[index])) < disten[index]:\n",
    "                    self.reward -= 2\n",
    "                    \n",
    "\n",
    "        if count == 2:\n",
    "            \n",
    "            \n",
    "            for index in enemies_inside:\n",
    "                \n",
    "                if np.abs(self.animal.distance(x_ini[index], y_ini[index])) > disten[index]:\n",
    "                    self.reward += 2\n",
    "                    \n",
    "                elif np.abs(self.animal.distance(x_ini[index], y_ini[index])) < disten[index]:\n",
    "                    self.reward -= 2\n",
    "                    \n",
    "        if count == 3:\n",
    "            \n",
    "            \n",
    "            for index in enemies_inside:\n",
    "                \n",
    "                \n",
    "                if np.abs(self.animal.distance(x_ini[index], y_ini[index])) > disten[index]:\n",
    "                    self.reward += 2\n",
    "                    \n",
    "                elif np.abs(self.animal.distance(x_ini[index], y_ini[index])) < disten[index]:\n",
    "                    self.reward -= 2\n",
    "                    \n",
    "        if count >= 4:\n",
    "            \n",
    "            if np.abs(self.animal.distance(x_en, y_en)) < disten[ind_e]:\n",
    "                self.reward -= 1\n",
    "            elif np.abs(self.animal.distance(x_en, y_en)) > disten[ind_e]:\n",
    "                self.reward += 2\n",
    "            \n",
    "            \n",
    "            \n",
    "            for index in enemies_inside:\n",
    "                \n",
    "                \n",
    "                if np.abs(self.animal.distance(x_ini[index], y_ini[index])) > disten[index]:\n",
    "                    self.reward += 1\n",
    "                    \n",
    "                elif np.abs(self.animal.distance(x_ini[index], y_ini[index])) < disten[index]:\n",
    "                    self.reward -= 1\n",
    "                    \n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "        state = [self.animal.xcor()/240, self.animal.ycor()/240,\n",
    "                (self.animal.xcor()-self.foods[ind_f].xcor())/240,\n",
    "                (self.animal.ycor()-self.foods[ind_f].ycor())/240]\n",
    "        \n",
    "\n",
    "        for ind in indices:\n",
    "            \n",
    "            if (np.abs(self.animal.xcor()-self.enemies[ind].xcor()) < 51. and np.abs(self.animal.ycor()-self.enemies[ind].ycor())< 51.):\n",
    "                \n",
    "                state.extend([(self.animal.xcor()-self.enemies[ind].xcor())/240, \n",
    "                            (self.animal.ycor()-self.enemies[ind].ycor())/240, self.enemies[ind].dir])\n",
    "            else:\n",
    "                state.extend([False, False, False])\n",
    "            \n",
    "            \n",
    "            \n",
    "        score = env.score\n",
    "        self.run_frame()\n",
    "        \n",
    "\n",
    "       \n",
    "        #print(self.enemies[ind].dir)\n",
    "        #print('COUNT', count)\n",
    "        #print(self.reward)\n",
    "        #print('Dopo',x_ini)\n",
    "        #print(state)\n",
    "        #print('indici dei nemici ',enemies_inside)\n",
    "        #print('Reward: ', self.reward)\n",
    "        #time.sleep(3)\n",
    "        #print(self.done)\n",
    "        return state, self.reward, self.done, score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a95d44e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DQN:\n",
    "\n",
    "    def __init__(self, action_space, state_space):\n",
    "\n",
    "        self.action_space = action_space\n",
    "        self.state_space = state_space\n",
    "        self.epsilon = 1\n",
    "        self.gamma = 0.6\n",
    "        self.batch_size = 64\n",
    "        self.epsilon_min = .01\n",
    "        self.epsilon_decay = .99999\n",
    "        self.learning_rate = 0.001\n",
    "        self.memory = deque(maxlen = 1000)\n",
    "        self.deaths = deque(maxlen = 100)\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Dense(64, input_shape=(self.state_space,), activation='tanh'))\n",
    "        model.add(Dense(128, activation='tanh'))\n",
    "        model.add(Dense(128, activation='tanh'))\n",
    "        model.add(Dense(64, activation='tanh'))\n",
    "        model.add(Dense(self.action_space, activation='linear'))\n",
    "        model.compile(loss='huber_loss', optimizer=Adam(learning_rate=self.learning_rate))\n",
    "    \n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        #print('STATO INIZIALE: ',state[0][6],'  ', state[0][10], '  STATO SUCCESSIVO: ', next_state[0][6],'  ', next_state[0][10])\n",
    "        #print('REWARD: ', reward)\n",
    "        #print('ACTION: ', action)\n",
    "       \n",
    "    def remember_prioritized(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        if done == True:\n",
    "            self.deaths.append((state, action, reward, next_state, done))\n",
    "            #print('stato: ', state, action, reward, next_state, done)\n",
    "        \n",
    "    def act(self, state):\n",
    "\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_space)\n",
    "        act_values = self.model.predict(state, verbose = 0)\n",
    "        return np.argmax(act_values[0])\n",
    "    \n",
    "    def act_trained(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # Random action\n",
    "            return random.randrange(self.action_space)\n",
    "        else:\n",
    "            # Greedy action\n",
    "            return np.argmax(self.model.predict(state, verbose = 0)[0])\n",
    "\n",
    "\n",
    "    def replay(self):\n",
    "\n",
    "        \n",
    "\n",
    "        if len(self.memory) < self.batch_size:\n",
    "\n",
    "            return\n",
    "        \n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "        states = np.array([i[0] for i in minibatch])\n",
    "        actions = np.array([i[1] for i in minibatch])\n",
    "        rewards = np.array([i[2] for i in minibatch])\n",
    "        next_states = np.array([i[3] for i in minibatch])\n",
    "        dones = np.array([i[4] for i in minibatch])\n",
    "\n",
    "        states = np.squeeze(states)\n",
    "        next_states = np.squeeze(next_states)\n",
    "        #print('squeezed vector', dones)\n",
    "        targets = rewards + self.gamma*(np.amax(self.model.predict_on_batch(next_states), axis=1))*(1-dones)\n",
    "        targets_full = self.model.predict_on_batch(states)\n",
    "\n",
    "        ind = np.array([i for i in range(self.batch_size)])\n",
    "        targets_full[[ind], [actions]] = targets\n",
    "\n",
    "        self.model.fit(states, targets_full, epochs=1, verbose=0,callbacks=[tensorboard_callback])\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        del minibatch\n",
    "        del states\n",
    "        del actions\n",
    "        del rewards\n",
    "        del next_states\n",
    "        del dones\n",
    "        del targets\n",
    "        del targets_full\n",
    "        \n",
    "    def replay_prioritized(self):\n",
    "        \n",
    "        p = 0.2\n",
    "        \n",
    "        if len(self.memory) < self.batch_size:\n",
    "\n",
    "            return\n",
    "        \n",
    "        minibatch = []\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            if (len(self.deaths) > 40) and (random.uniform(0,1) < p):\n",
    "                minibatch.append(random.sample(self.deaths, 1)[0])\n",
    "                \n",
    "            else:\n",
    "                minibatch.append(random.sample(self.memory, 1)[0])\n",
    "                \n",
    "        states = np.array([i[0] for i in minibatch])\n",
    "        actions = np.array([i[1] for i in minibatch])\n",
    "        rewards = np.array([i[2] for i in minibatch])\n",
    "        next_states = np.array([i[3] for i in minibatch])\n",
    "        dones = np.array([i[4] for i in minibatch])\n",
    "\n",
    "        states = np.squeeze(states)\n",
    "        next_states = np.squeeze(next_states)\n",
    "\n",
    "        targets = rewards + self.gamma*(np.amax(self.model.predict_on_batch(next_states), axis=1))*(1-dones)\n",
    "        targets_full = self.model.predict_on_batch(states)\n",
    "\n",
    "        ind = np.array([i for i in range(self.batch_size)])\n",
    "        targets_full[[ind], [actions]] = targets\n",
    "\n",
    "        self.model.fit(states, targets_full, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "    def save_agent(self, file_path):\n",
    "       \n",
    "        self.model.save(file_path + \"_model.h5\")\n",
    "    \n",
    "        with open(file_path + \"_epsilon.pkl\", \"wb\") as f:\n",
    "            pickle.dump(self.epsilon, f)\n",
    "    \n",
    "    @classmethod\n",
    "    \n",
    "    def load_agent(cls, action_space, state_space, file_path):\n",
    "        \n",
    "        loaded_model = tf.keras.models.load_model(file_path + \"_model.h5\")\n",
    "\n",
    "        new_agent = cls(action_space=action_space, state_space=state_space)\n",
    "\n",
    "        new_agent.model = loaded_model\n",
    "\n",
    "        return new_agent\n",
    "\n",
    "    \n",
    "def log_training_progress(episode, loss, epsilon, loss_history):\n",
    "    # Log training progress, including episode number, loss, and epsilon\n",
    "    print(\"Episode: {:5d}, Loss: {:.5f}, Epsilon: {:.5f}\".format(episode, loss, epsilon))\n",
    "    loss_history.append(loss)  # Append the loss value to the history    \n",
    "    \n",
    "def train_dqn(episode, save_interval = 50):\n",
    "\n",
    "    loss = []\n",
    "    loss.append(0.0)\n",
    "    SCORE = []\n",
    "    loss_history = []\n",
    "\n",
    "    action_space = 4\n",
    "    state_space = 10\n",
    "    max_steps = 1000\n",
    "\n",
    "    agent = DQN(action_space, state_space)\n",
    "    \n",
    "    for e in range(episode):\n",
    "        state = env.reset_good()\n",
    "        state = np.reshape(state, (1, state_space))\n",
    "        score = 0\n",
    "        log_training_progress(e, loss[-1], agent.epsilon, loss_history)\n",
    "        \n",
    "        for i in range(max_steps):\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, Score = env.step_good(action)\n",
    "            \n",
    "            score += reward\n",
    "            next_state = np.reshape(next_state, (1, state_space))\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            agent.replay()\n",
    "            if done:\n",
    "                print(\"episode: {}/{}, score: {}\".format(e, episode, score))\n",
    "                #print(agent.memory[-1])\n",
    "                break\n",
    "        loss.append(score)\n",
    "        SCORE.append(Score)\n",
    "        #monitor memory usage\n",
    "          # monitor_memory()\n",
    "        \n",
    "        if e % save_interval == 0:\n",
    "            save_path = \"models/episode_{}.h5\".format(e)\n",
    "            agent.save_agent(save_path)\n",
    "            print(\"Saved model at episode {} to {}\".format(e, save_path))\n",
    "    \n",
    "            \n",
    "    plt.plot(range(len(loss_history)), loss_history, label='Loss')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Progress')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(range(len(SCORE)), SCORE, label='SCORE')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('score')\n",
    "    plt.title('Training score Progress')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return agent, loss_history\n",
    "\n",
    "def train_dqn_prioritized(episode, save_interval = 100):\n",
    "\n",
    "    loss = []\n",
    "    loss.append(0.0)\n",
    "    loss_history = []\n",
    "\n",
    "    action_space = 4\n",
    "    state_space = 16\n",
    "    max_steps = 1000\n",
    "\n",
    "    agent = DQN(action_space, state_space)\n",
    "    \n",
    "    for e in range(episode):\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, (1, state_space))\n",
    "        score = 0\n",
    "        log_training_progress(e, loss[-1], agent.epsilon, loss_history)\n",
    "        \n",
    "        for i in range(max_steps):\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            score += reward\n",
    "            next_state = np.reshape(next_state, (1, state_space))\n",
    "            agent.remember_prioritized(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            agent.replay_prioritized()\n",
    "            if done:\n",
    "                print(\"episode: {}/{}, score: {}\".format(e, episode, score))\n",
    "                #print(agent.memory[-1])\n",
    "                break\n",
    "        loss.append(score)\n",
    "        if e % save_interval == 0:\n",
    "            save_path = \"models/model_episode_{}.h5\".format(e)\n",
    "            agent.save_agent(save_path)\n",
    "            print(\"Saved model at episode {} to {}\".format(e, save_path))\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            \n",
    "    plt.plot(range(len(loss_history)), loss_history, label='Loss')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Progress')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return agent, loss\n",
    "\n",
    "def train_dqn_again(episode, agent, save_interval = 100):\n",
    "\n",
    "    loss = []\n",
    "    loss.append(0.0)\n",
    "    loss_history = []\n",
    "    SCORE = []\n",
    "    action_space = 4\n",
    "    state_space = 10\n",
    "    max_steps = 1000\n",
    "    \n",
    "    for e in range(episode):\n",
    "        state = env.reset_good()\n",
    "        state = np.reshape(state, (1, state_space))\n",
    "        score = 0\n",
    "        log_training_progress(e, loss[-1], agent.epsilon, loss_history)\n",
    "        \n",
    "        for i in range(max_steps):\n",
    "            \n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, Score = env.step_good(action)\n",
    "            score += reward\n",
    "            next_state = np.reshape(next_state, (1, state_space))\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            #print('stato  ',state)\n",
    "            #print('stato successivo  ',next_state)\n",
    "            #print('Reward trained: ', reward)\n",
    "            agent.replay()\n",
    "            state = next_state\n",
    "            del action\n",
    "            del reward\n",
    "            \n",
    "            \n",
    "            if done:\n",
    "                print(\"episode: {}/{}, score: {}\".format(e, episode, score))\n",
    "                break\n",
    "        #monitor_memory()\n",
    "        loss.append(score)\n",
    "        SCORE.append(Score)\n",
    "        #monitor memory usage\n",
    "        \n",
    "        \n",
    "        if e % save_interval == 0:\n",
    "            save_path = \"models/model_episode_{}.h5\".format(e)\n",
    "            agent.save_agent(save_path)\n",
    "            print(\"Saved model at episode {} to {}\".format(e, save_path))\n",
    "            \n",
    "            \n",
    "    plt.plot(range(len(loss_history)), loss_history, label='Loss')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Progress')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(range(len(SCORE)), SCORE,'o', label='SCORE')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('score')\n",
    "    plt.title('Training score Progress')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    print(np.mean(SCORE))\n",
    "    \n",
    "    return agent, loss_history\n",
    "\n",
    "\n",
    "def train_dqn_again_prioritized(episode, agent, save_interval = 100):\n",
    "\n",
    "    loss = []\n",
    "    loss.append(0.0)\n",
    "    loss_history = []\n",
    "    SCORE = []\n",
    "    action_space = 4\n",
    "    state_space = 10\n",
    "    max_steps = 1000\n",
    "    \n",
    "    for e in range(episode):\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, (1, state_space))\n",
    "        score = 0\n",
    "        log_training_progress(e, loss[-1], agent.epsilon, loss_history)\n",
    "        for i in range(max_steps):\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, Score = env.step(action)\n",
    "            score += reward\n",
    "            next_state = np.reshape(next_state, (1, state_space))\n",
    "            agent.remember_prioritized(state, action, reward, next_state, done)\n",
    "            #print('stato  ',state)\n",
    "            #print('stato successivo  ',next_state)\n",
    "            #print('Reward trained: ', reward)\n",
    "            agent.replay_prioritized()\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                print(\"episode: {}/{}, score: {}\".format(e, episode, score))\n",
    "                break\n",
    "        loss.append(score)\n",
    "        SCORE.append(Score)\n",
    "        if e % save_interval == 0:\n",
    "            save_path = \"models/model_episode_{}.h5\".format(e)\n",
    "            agent.save_agent(save_path)\n",
    "            print(\"Saved model at episode {} to {}\".format(e, save_path))\n",
    "            \n",
    "            \n",
    "    plt.plot(range(len(loss_history)), loss_history, label='Loss')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Progress')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    plt.plot(range(len(SCORE)), SCORE,'o', label='SCORE')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('score')\n",
    "    plt.title('Training score Progress')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return agent, loss_history\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test_dqn(episode, agent):\n",
    "    action_space = 4\n",
    "    state_space = 16\n",
    "    max_steps = 1000\n",
    "\n",
    "    for e in range(episode):\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, (1, state_space))\n",
    "        score = 0\n",
    "        for i in range(max_steps):\n",
    "            action = agent.act_trained(state)\n",
    "            next_state, reward, done, Score = env.step(action)\n",
    "            score += reward\n",
    "            next_state = np.reshape(next_state, (1, state_space))\n",
    "            state = next_state\n",
    "            if done:\n",
    "                print(\"episode: {}/{}, score: {}\".format(e, episode, score))\n",
    "                break\n",
    "        monitor_memory()\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a292fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TurtleGame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8601c30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#agent, loss = train_dqn(300, 100)\n",
    "#agent.save_agent('agent_back')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a51298f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_trained = DQN.load_agent(action_space = 4, state_space = 16, file_path = 'agent_back_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40eca0b4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:     0, Loss: 0.00000, Epsilon: 0.01000\n",
      "Score:  8\n",
      "episode: 0/300, score: 1119.0000000000055\n",
      "Saved model at episode 0 to models/model_episode_0.h5\n",
      "Episode:     1, Loss: 1119.00000, Epsilon: 0.01000\n",
      "Score:  4\n",
      "episode: 1/300, score: 468.2\n",
      "Episode:     2, Loss: 468.20000, Epsilon: 0.01000\n"
     ]
    },
    {
     "ename": "TurtleGraphicsError",
     "evalue": "bad color string: red",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTurtleGraphicsError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m agent_trained_1, loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dqn_again\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent_trained\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_interval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 290\u001b[0m, in \u001b[0;36mtrain_dqn_again\u001b[1;34m(episode, agent, save_interval)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_steps):\n\u001b[0;32m    289\u001b[0m     action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mact(state)\n\u001b[1;32m--> 290\u001b[0m     next_state, reward, done, Score \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_good\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    291\u001b[0m     score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m    292\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(next_state, (\u001b[38;5;241m1\u001b[39m, state_space))\n",
      "Cell \u001b[1;32mIn[4], line 548\u001b[0m, in \u001b[0;36mTurtleGame.step_good\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    546\u001b[0m     dist_x\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manimal\u001b[38;5;241m.\u001b[39mxcor()\u001b[38;5;241m-\u001b[39men\u001b[38;5;241m.\u001b[39mxcor())\n\u001b[0;32m    547\u001b[0m     dist_y\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manimal\u001b[38;5;241m.\u001b[39mycor()\u001b[38;5;241m-\u001b[39men\u001b[38;5;241m.\u001b[39mycor())\n\u001b[1;32m--> 548\u001b[0m     \u001b[43men\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mred\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    551\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manimal_left()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow_env\\lib\\turtle.py:2217\u001b[0m, in \u001b[0;36mTPen.color\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   2215\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m l \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m   2216\u001b[0m     pcolor \u001b[38;5;241m=\u001b[39m fcolor \u001b[38;5;241m=\u001b[39m args\n\u001b[1;32m-> 2217\u001b[0m pcolor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_colorstr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpcolor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2218\u001b[0m fcolor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_colorstr(fcolor)\n\u001b[0;32m   2219\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpen(pencolor\u001b[38;5;241m=\u001b[39mpcolor, fillcolor\u001b[38;5;241m=\u001b[39mfcolor)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow_env\\lib\\turtle.py:2697\u001b[0m, in \u001b[0;36mRawTurtle._colorstr\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m   2696\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_colorstr\u001b[39m(\u001b[38;5;28mself\u001b[39m, args):\n\u001b[1;32m-> 2697\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscreen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_colorstr\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow_env\\lib\\turtle.py:1159\u001b[0m, in \u001b[0;36mTurtleScreen._colorstr\u001b[1;34m(self, color)\u001b[0m\n\u001b[0;32m   1157\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m color\n\u001b[0;32m   1158\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1159\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TurtleGraphicsError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbad color string: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mstr\u001b[39m(color))\n\u001b[0;32m   1160\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1161\u001b[0m     r, g, b \u001b[38;5;241m=\u001b[39m color\n",
      "\u001b[1;31mTurtleGraphicsError\u001b[0m: bad color string: red"
     ]
    }
   ],
   "source": [
    "#agent_trained_1, loss = train_dqn_again(300, agent_trained, save_interval = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f51f4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent_trained_1.save_agent('agent_back_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed0960f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b599f4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  1\n",
      "episode: 0/10, score: 109.0\n",
      "RAM Usage: 2.47%\n",
      "Disk Space Used: 43.60%\n",
      "Score:  0\n",
      "episode: 1/10, score: 92.0\n",
      "RAM Usage: 2.51%\n",
      "Disk Space Used: 43.60%\n",
      "Score:  0\n",
      "episode: 2/10, score: -8.0\n",
      "RAM Usage: 2.52%\n",
      "Disk Space Used: 43.60%\n",
      "Score:  0\n",
      "episode: 3/10, score: -34.0\n",
      "RAM Usage: 2.52%\n",
      "Disk Space Used: 43.60%\n",
      "Score:  0\n",
      "episode: 4/10, score: -57.0\n",
      "RAM Usage: 2.54%\n",
      "Disk Space Used: 43.60%\n",
      "Score:  0\n",
      "episode: 5/10, score: -11.0\n",
      "RAM Usage: 2.54%\n",
      "Disk Space Used: 43.60%\n",
      "Score:  0\n",
      "episode: 6/10, score: -43.0\n",
      "RAM Usage: 2.54%\n",
      "Disk Space Used: 43.60%\n",
      "Score:  0\n",
      "episode: 7/10, score: -33.0\n",
      "RAM Usage: 2.54%\n",
      "Disk Space Used: 43.60%\n"
     ]
    },
    {
     "ename": "TurtleGraphicsError",
     "evalue": "bad color string: red",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTurtleGraphicsError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtest_dqn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43magent_trained\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 402\u001b[0m, in \u001b[0;36mtest_dqn\u001b[1;34m(episode, agent)\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_steps):\n\u001b[0;32m    401\u001b[0m     action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mact_trained(state)\n\u001b[1;32m--> 402\u001b[0m     next_state, reward, done, Score \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    403\u001b[0m     score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m    404\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(next_state, (\u001b[38;5;241m1\u001b[39m, state_space))\n",
      "Cell \u001b[1;32mIn[4], line 388\u001b[0m, in \u001b[0;36mTurtleGame.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    386\u001b[0m     x_in\u001b[38;5;241m.\u001b[39mappend(en\u001b[38;5;241m.\u001b[39mxcor())\n\u001b[0;32m    387\u001b[0m     y_in\u001b[38;5;241m.\u001b[39mappend(en\u001b[38;5;241m.\u001b[39mycor())\n\u001b[1;32m--> 388\u001b[0m     \u001b[43men\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mred\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manimal_left()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow_env\\lib\\turtle.py:2217\u001b[0m, in \u001b[0;36mTPen.color\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   2215\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m l \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m   2216\u001b[0m     pcolor \u001b[38;5;241m=\u001b[39m fcolor \u001b[38;5;241m=\u001b[39m args\n\u001b[1;32m-> 2217\u001b[0m pcolor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_colorstr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpcolor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2218\u001b[0m fcolor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_colorstr(fcolor)\n\u001b[0;32m   2219\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpen(pencolor\u001b[38;5;241m=\u001b[39mpcolor, fillcolor\u001b[38;5;241m=\u001b[39mfcolor)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow_env\\lib\\turtle.py:2697\u001b[0m, in \u001b[0;36mRawTurtle._colorstr\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m   2696\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_colorstr\u001b[39m(\u001b[38;5;28mself\u001b[39m, args):\n\u001b[1;32m-> 2697\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscreen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_colorstr\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow_env\\lib\\turtle.py:1159\u001b[0m, in \u001b[0;36mTurtleScreen._colorstr\u001b[1;34m(self, color)\u001b[0m\n\u001b[0;32m   1157\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m color\n\u001b[0;32m   1158\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1159\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TurtleGraphicsError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbad color string: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mstr\u001b[39m(color))\n\u001b[0;32m   1160\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1161\u001b[0m     r, g, b \u001b[38;5;241m=\u001b[39m color\n",
      "\u001b[1;31mTurtleGraphicsError\u001b[0m: bad color string: red"
     ]
    }
   ],
   "source": [
    "test_dqn(10,agent_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe87eb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
